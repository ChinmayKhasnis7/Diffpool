{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiffPool_Benchmark_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChinmayKhasnis7/GNN-Differentiable_Pooling/blob/master/DiffPool_Benchmark_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIYY2RY-_DnX",
        "colab_type": "code",
        "outputId": "0ae0af58-1308-4ba3-ee6d-5714ca10da74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pip install torch==1.5.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cluHwdbznrnR",
        "colab_type": "code",
        "outputId": "7e436d9b-f416-4c46-d4f9-3fcb4a14eb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCU9L4uzkzC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yiBjPiMjhfi",
        "colab_type": "code",
        "outputId": "c267acdd-0d17-4994-d1c4-3b76543a36d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpOzM3lrnLX4",
        "colab_type": "code",
        "outputId": "d3f756b7-660c-415a-da9a-02ba290644ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ow1cpbPjmqn",
        "colab_type": "code",
        "outputId": "89dcfa66-0169-4cc6-fb83-4d01e40dcfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.backends.cudnn.enabled "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOshm6Objs60",
        "colab_type": "code",
        "outputId": "7a70b6bf-392e-43c3-e50f-b66e8b6144c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELHHe4rtTh2m",
        "colab_type": "code",
        "outputId": "e00bdd3a-808f-4f44-f4f0-21866f20dc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/diffpool-master"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/diffpool-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAZX5aQ5oyL3",
        "colab_type": "code",
        "outputId": "63223c97-7f7e-4663-cfcc-402239154a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " aggregators.py\t\t\t     graph_embedding.py   __pycache__\n",
            "'Copy of DiffPool_bmdataset.ipynb'   graphsage.py\t  README.md\n",
            " cross_val.py\t\t\t     graph_sampler.py\t  set2set.py\n",
            " data\t\t\t\t     LICENSE\t\t  train.py\n",
            " encoders.py\t\t\t     load_data.py\t  util.py\n",
            " example.sh\t\t\t     log\n",
            " gen\t\t\t\t     partition.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTOdeQopWL5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #aggregator.py\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Set of modules for aggregating embeddings of neighbors.\n",
        "\"\"\"\n",
        "\n",
        "class MeanAggregator(nn.Module):\n",
        "    \"\"\"\n",
        "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, features, cuda=False, gcn=False): \n",
        "        \"\"\"\n",
        "        Initializes the aggregator for a specific graph.\n",
        "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
        "        cuda -- whether to use GPU\n",
        "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
        "        \"\"\"\n",
        "\n",
        "        super(MeanAggregator, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.cuda = cuda\n",
        "        self.gcn = gcn\n",
        "        \n",
        "    def forward(self, nodes, to_neighs, num_sample=10):\n",
        "        \"\"\"\n",
        "        nodes --- list of nodes in a batch\n",
        "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
        "        num_sample --- number of neighbors to sample. No sampling if None.\n",
        "        \"\"\"\n",
        "        # Local pointers to functions (speed hack)\n",
        "        _set = set\n",
        "        if not num_sample is None:\n",
        "            _sample = random.sample\n",
        "            samp_neighs = [_set(_sample(to_neigh, \n",
        "                            num_sample,\n",
        "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
        "        else:\n",
        "            samp_neighs = to_neighs\n",
        "\n",
        "        if self.gcn:\n",
        "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
        "        unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
        "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        mask[row_indices, column_indices] = 1\n",
        "        if self.cuda:\n",
        "            mask = mask.cuda()\n",
        "        num_neigh = mask.sum(1, keepdim=True)\n",
        "        mask = mask.div(num_neigh)\n",
        "        if self.cuda:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
        "        else:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
        "        to_feats = mask.mm(embed_matrix)\n",
        "        return to_feats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTlchfz3WdlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx #crossval.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from graph_sampler import GraphSampler\n",
        "\n",
        "def prepare_val_data(graphs, args, val_idx, max_nodes=0):\n",
        "\n",
        "    random.shuffle(graphs)\n",
        "    val_size = len(graphs) // 10\n",
        "    train_graphs = graphs[:val_idx * val_size]\n",
        "    if val_idx < 9:\n",
        "        train_graphs = train_graphs + graphs[(val_idx+1) * val_size :]\n",
        "    val_graphs = graphs[val_idx*val_size: (val_idx+1)*val_size]\n",
        "    print('Num training graphs: ', len(train_graphs), \n",
        "          '; Num validation graphs: ', len(val_graphs))\n",
        "\n",
        "    print('Number of graphs: ', len(graphs))\n",
        "    print('Number of edges: ', sum([G.number_of_edges() for G in graphs]))\n",
        "    print('Max, avg, std of graph size: ', \n",
        "            max([G.number_of_nodes() for G in graphs]), ', '\n",
        "            \"{0:.2f}\".format(np.mean([G.number_of_nodes() for G in graphs])), ', '\n",
        "            \"{0:.2f}\".format(np.std([G.number_of_nodes() for G in graphs])))\n",
        "\n",
        "    # minibatch\n",
        "    dataset_sampler = GraphSampler(train_graphs, normalize=False, max_num_nodes=max_nodes,\n",
        "            features=args.feature_type)\n",
        "    train_dataset_loader = torch.utils.data.DataLoader(\n",
        "            dataset_sampler, \n",
        "            batch_size=args.batch_size, \n",
        "            shuffle=True,\n",
        "            num_workers=args.num_workers)\n",
        "\n",
        "    dataset_sampler = GraphSampler(val_graphs, normalize=False, max_num_nodes=max_nodes,\n",
        "            features=args.feature_type)\n",
        "    val_dataset_loader = torch.utils.data.DataLoader(\n",
        "            dataset_sampler, \n",
        "            batch_size=args.batch_size, \n",
        "            shuffle=False,\n",
        "            num_workers=args.num_workers)\n",
        "\n",
        "    return train_dataset_loader, val_dataset_loader, \\\n",
        "            dataset_sampler.max_num_nodes, dataset_sampler.feat_dim, dataset_sampler.assign_feat_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-ryHRrsW0iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #encoders.py\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from set2set import Set2Set\n",
        "\n",
        "# GCN basic operation\n",
        "class GraphConv(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, add_self=False, normalize_embedding=False,\n",
        "            dropout=0.0, bias=True):\n",
        "        super(GraphConv, self).__init__()\n",
        "        self.add_self = add_self\n",
        "        self.dropout = dropout\n",
        "        if dropout > 0.001:\n",
        "            self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.normalize_embedding = normalize_embedding\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim).cuda())\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(output_dim).cuda())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.dropout > 0.001:\n",
        "            x = self.dropout_layer(x)\n",
        "        y = torch.matmul(adj, x)\n",
        "        if self.add_self:\n",
        "            y += x\n",
        "        y = torch.matmul(y,self.weight)\n",
        "        if self.bias is not None:\n",
        "            y = y + self.bias\n",
        "        if self.normalize_embedding:\n",
        "            y = F.normalize(y, p=2, dim=2)\n",
        "            #print(y[0][0])\n",
        "        return y\n",
        "\n",
        "class GcnEncoderGraph(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, embedding_dim, label_dim, num_layers,\n",
        "            pred_hidden_dims=[], concat=True, bn=True, dropout=0.0, args=None):\n",
        "        super(GcnEncoderGraph, self).__init__()\n",
        "        self.concat = concat\n",
        "        add_self = not concat\n",
        "        self.bn = bn\n",
        "        self.num_layers = num_layers\n",
        "        self.num_aggs=1\n",
        "\n",
        "        self.bias = True\n",
        "        if args is not None:\n",
        "            self.bias = args.bias\n",
        "\n",
        "        self.conv_first, self.conv_block, self.conv_last = self.build_conv_layers(\n",
        "                input_dim, hidden_dim, embedding_dim, num_layers, \n",
        "                add_self, normalize=True, dropout=dropout)\n",
        "        self.act = nn.ReLU()\n",
        "        self.label_dim = label_dim\n",
        "\n",
        "        if concat:\n",
        "            self.pred_input_dim = hidden_dim * (num_layers - 1) + embedding_dim\n",
        "        else:\n",
        "            self.pred_input_dim = embedding_dim\n",
        "        self.pred_model = self.build_pred_layers(self.pred_input_dim, pred_hidden_dims, \n",
        "                label_dim, num_aggs=self.num_aggs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, GraphConv):\n",
        "                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data = init.constant(m.bias.data, 0.0)\n",
        "\n",
        "    def build_conv_layers(self, input_dim, hidden_dim, embedding_dim, num_layers, add_self,\n",
        "            normalize=False, dropout=0.0):\n",
        "        conv_first = GraphConv(input_dim=input_dim, output_dim=hidden_dim, add_self=add_self,\n",
        "                normalize_embedding=normalize, bias=self.bias)\n",
        "        conv_block = nn.ModuleList(\n",
        "                [GraphConv(input_dim=hidden_dim, output_dim=hidden_dim, add_self=add_self,\n",
        "                        normalize_embedding=normalize, dropout=dropout, bias=self.bias) \n",
        "                 for i in range(num_layers-2)])\n",
        "        conv_last = GraphConv(input_dim=hidden_dim, output_dim=embedding_dim, add_self=add_self,\n",
        "                normalize_embedding=normalize, bias=self.bias)\n",
        "        return conv_first, conv_block, conv_last\n",
        "\n",
        "    def build_pred_layers(self, pred_input_dim, pred_hidden_dims, label_dim, num_aggs=1):\n",
        "        pred_input_dim = pred_input_dim * num_aggs\n",
        "        if len(pred_hidden_dims) == 0:\n",
        "            pred_model = nn.Linear(pred_input_dim, label_dim)\n",
        "        else:\n",
        "            pred_layers = []\n",
        "            for pred_dim in pred_hidden_dims:\n",
        "                pred_layers.append(nn.Linear(pred_input_dim, pred_dim))\n",
        "                pred_layers.append(self.act)\n",
        "                pred_input_dim = pred_dim\n",
        "            pred_layers.append(nn.Linear(pred_dim, label_dim))\n",
        "            pred_model = nn.Sequential(*pred_layers)\n",
        "        return pred_model\n",
        "\n",
        "    def construct_mask(self, max_nodes, batch_num_nodes): \n",
        "        ''' For each num_nodes in batch_num_nodes, the first num_nodes entries of the \n",
        "        corresponding column are 1's, and the rest are 0's (to be masked out).\n",
        "        Dimension of mask: [batch_size x max_nodes x 1]\n",
        "        '''\n",
        "        # masks\n",
        "        packed_masks = [torch.ones(int(num)) for num in batch_num_nodes]\n",
        "        batch_size = len(batch_num_nodes)\n",
        "        out_tensor = torch.zeros(batch_size, max_nodes)\n",
        "        for i, mask in enumerate(packed_masks):\n",
        "            out_tensor[i, :batch_num_nodes[i]] = mask\n",
        "        return out_tensor.unsqueeze(2).cuda()\n",
        "\n",
        "    def apply_bn(self, x):\n",
        "        ''' Batch normalization of 3D tensor x\n",
        "        '''\n",
        "        bn_module = nn.BatchNorm1d(x.size()[1]).cuda()\n",
        "        return bn_module(x)\n",
        "\n",
        "    def gcn_forward(self, x, adj, conv_first, conv_block, conv_last, embedding_mask=None):\n",
        "\n",
        "        ''' Perform forward prop with graph convolution.\n",
        "        Returns:\n",
        "            Embedding matrix with dimension [batch_size x num_nodes x embedding]\n",
        "        '''\n",
        "\n",
        "        x = conv_first(x, adj)\n",
        "        x = self.act(x)\n",
        "        if self.bn:\n",
        "            x = self.apply_bn(x)\n",
        "        x_all = [x]\n",
        "        #out_all = []\n",
        "        #out, _ = torch.max(x, dim=1)\n",
        "        #out_all.append(out)\n",
        "        for i in range(len(conv_block)):\n",
        "            x = conv_block[i](x,adj)\n",
        "            x = self.act(x)\n",
        "            if self.bn:\n",
        "                x = self.apply_bn(x)\n",
        "            x_all.append(x)\n",
        "        x = conv_last(x,adj)\n",
        "        x_all.append(x)\n",
        "        # x_tensor: [batch_size x num_nodes x embedding]\n",
        "        x_tensor = torch.cat(x_all, dim=2)\n",
        "        if embedding_mask is not None:\n",
        "            x_tensor = x_tensor * embedding_mask\n",
        "        return x_tensor\n",
        "\n",
        "    def forward(self, x, adj, batch_num_nodes=None, **kwargs):\n",
        "        # mask\n",
        "        max_num_nodes = adj.size()[1]\n",
        "        if batch_num_nodes is not None:\n",
        "            self.embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
        "        else:\n",
        "            self.embedding_mask = None\n",
        "\n",
        "        # conv\n",
        "        x = self.conv_first(x, adj)\n",
        "        x = self.act(x)\n",
        "        if self.bn:\n",
        "            x = self.apply_bn(x)\n",
        "        out_all = []\n",
        "        out, _ = torch.max(x, dim=1)\n",
        "        out_all.append(out)\n",
        "        for i in range(self.num_layers-2):\n",
        "            x = self.conv_block[i](x,adj)\n",
        "            x = self.act(x)\n",
        "            if self.bn:\n",
        "                x = self.apply_bn(x)\n",
        "            out,_ = torch.max(x, dim=1)\n",
        "            out_all.append(out)\n",
        "            if self.num_aggs == 2:\n",
        "                out = torch.sum(x, dim=1)\n",
        "                out_all.append(out)\n",
        "        x = self.conv_last(x,adj)\n",
        "        #x = self.act(x)\n",
        "        out, _ = torch.max(x, dim=1)\n",
        "        out_all.append(out)\n",
        "        if self.num_aggs == 2:\n",
        "            out = torch.sum(x, dim=1)\n",
        "            out_all.append(out)\n",
        "        if self.concat:\n",
        "            output = torch.cat(out_all, dim=1)\n",
        "        else:\n",
        "            output = out\n",
        "        ypred = self.pred_model(output)\n",
        "        #print(output.size())\n",
        "        return ypred\n",
        "\n",
        "    def loss(self, pred, label, type='softmax'):\n",
        "        # softmax + CE\n",
        "        if type == 'softmax':\n",
        "            return F.cross_entropy(pred, label, reduction='mean')\n",
        "        elif type == 'margin':\n",
        "            batch_size = pred.size()[0]\n",
        "            label_onehot = torch.zeros(batch_size, self.label_dim).long().cuda()\n",
        "            label_onehot.scatter_(1, label.view(-1,1), 1)\n",
        "            return torch.nn.MultiLabelMarginLoss()(pred, label_onehot)\n",
        "            \n",
        "        #return F.binary_cross_entropy(F.sigmoid(pred[:,0]), label.float())\n",
        "\n",
        "\n",
        "class GcnSet2SetEncoder(GcnEncoderGraph):\n",
        "    def __init__(self, input_dim, hidden_dim, embedding_dim, label_dim, num_layers,\n",
        "            pred_hidden_dims=[], concat=True, bn=True, dropout=0.0, args=None):\n",
        "        super(GcnSet2SetEncoder, self).__init__(input_dim, hidden_dim, embedding_dim, label_dim,\n",
        "                num_layers, pred_hidden_dims, concat, bn, dropout, args=args)\n",
        "        self.s2s = Set2Set(self.pred_input_dim, self.pred_input_dim * 2)\n",
        "\n",
        "    def forward(self, x, adj, batch_num_nodes=None, **kwargs):\n",
        "        # mask\n",
        "        max_num_nodes = adj.size()[1]\n",
        "        if batch_num_nodes is not None:\n",
        "            embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
        "        else:\n",
        "            embedding_mask = None\n",
        "\n",
        "        embedding_tensor = self.gcn_forward(x, adj,\n",
        "                self.conv_first, self.conv_block, self.conv_last, embedding_mask)\n",
        "        out = self.s2s(embedding_tensor)\n",
        "        #out, _ = torch.max(embedding_tensor, dim=1)\n",
        "        ypred = self.pred_model(out)\n",
        "        return ypred\n",
        "\n",
        "\n",
        "class SoftPoolingGcnEncoder(GcnEncoderGraph):\n",
        "    def __init__(self, max_num_nodes, input_dim, hidden_dim, embedding_dim, label_dim, num_layers,\n",
        "            assign_hidden_dim, assign_ratio=0.25, assign_num_layers=-1, num_pooling=1,\n",
        "            pred_hidden_dims=[50], concat=True, bn=True, dropout=0.0, linkpred=True,\n",
        "            assign_input_dim=-1, args=None):\n",
        "        '''\n",
        "        Args:\n",
        "            num_layers: number of gc layers before each pooling\n",
        "            num_nodes: number of nodes for each graph in batch\n",
        "            linkpred: flag to turn on link prediction side objective\n",
        "        '''\n",
        "\n",
        "        super(SoftPoolingGcnEncoder, self).__init__(input_dim, hidden_dim, embedding_dim, label_dim,\n",
        "                num_layers, pred_hidden_dims=pred_hidden_dims, concat=concat, args=args)\n",
        "        add_self = not concat\n",
        "        self.num_pooling = num_pooling\n",
        "        self.linkpred = linkpred\n",
        "        self.assign_ent = True\n",
        "\n",
        "        # GC\n",
        "        self.conv_first_after_pool = nn.ModuleList()\n",
        "        self.conv_block_after_pool = nn.ModuleList()\n",
        "        self.conv_last_after_pool = nn.ModuleList()\n",
        "        for i in range(num_pooling):\n",
        "            # use self to register the modules in self.modules()\n",
        "            conv_first2, conv_block2, conv_last2 = self.build_conv_layers(\n",
        "                    self.pred_input_dim, hidden_dim, embedding_dim, num_layers, \n",
        "                    add_self, normalize=True, dropout=dropout)\n",
        "            self.conv_first_after_pool.append(conv_first2)\n",
        "            self.conv_block_after_pool.append(conv_block2)\n",
        "            self.conv_last_after_pool.append(conv_last2)\n",
        "\n",
        "        # assignment\n",
        "        assign_dims = []\n",
        "        if assign_num_layers == -1:\n",
        "            assign_num_layers = num_layers\n",
        "        if assign_input_dim == -1:\n",
        "            assign_input_dim = input_dim\n",
        "\n",
        "        self.assign_conv_first_modules = nn.ModuleList()\n",
        "        self.assign_conv_block_modules = nn.ModuleList()\n",
        "        self.assign_conv_last_modules = nn.ModuleList()\n",
        "        self.assign_pred_modules = nn.ModuleList()\n",
        "        assign_dim = int(max_num_nodes * assign_ratio)\n",
        "        for i in range(num_pooling):\n",
        "            assign_dims.append(assign_dim)\n",
        "            assign_conv_first, assign_conv_block, assign_conv_last = self.build_conv_layers(\n",
        "                    assign_input_dim, assign_hidden_dim, assign_dim, assign_num_layers, add_self,\n",
        "                    normalize=True)\n",
        "            assign_pred_input_dim = assign_hidden_dim * (num_layers - 1) + assign_dim if concat else assign_dim\n",
        "            assign_pred = self.build_pred_layers(assign_pred_input_dim, [], assign_dim, num_aggs=1)\n",
        "\n",
        "\n",
        "            # next pooling layer\n",
        "            assign_input_dim = self.pred_input_dim\n",
        "            assign_dim = int(assign_dim * assign_ratio)\n",
        "\n",
        "            self.assign_conv_first_modules.append(assign_conv_first)\n",
        "            self.assign_conv_block_modules.append(assign_conv_block)\n",
        "            self.assign_conv_last_modules.append(assign_conv_last)\n",
        "            self.assign_pred_modules.append(assign_pred)\n",
        "\n",
        "        self.pred_model = self.build_pred_layers(self.pred_input_dim * (num_pooling+1), pred_hidden_dims, \n",
        "                label_dim, num_aggs=self.num_aggs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, GraphConv):\n",
        "                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data = init.constant(m.bias.data, 0.0)\n",
        "\n",
        "    def forward(self, x, adj, batch_num_nodes, **kwargs):\n",
        "        if 'assign_x' in kwargs:\n",
        "            x_a = kwargs['assign_x']\n",
        "        else:\n",
        "            x_a = x\n",
        "\n",
        "        # mask\n",
        "        max_num_nodes = adj.size()[1]\n",
        "        if batch_num_nodes is not None:\n",
        "            embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
        "        else:\n",
        "            embedding_mask = None\n",
        "\n",
        "        out_all = []\n",
        "\n",
        "        #self.assign_tensor = self.gcn_forward(x_a, adj, \n",
        "        #        self.assign_conv_first_modules[0], self.assign_conv_block_modules[0], self.assign_conv_last_modules[0],\n",
        "        #        embedding_mask)\n",
        "        ## [batch_size x num_nodes x next_lvl_num_nodes]\n",
        "        #self.assign_tensor = nn.Softmax(dim=-1)(self.assign_pred(self.assign_tensor))\n",
        "        #if embedding_mask is not None:\n",
        "        #    self.assign_tensor = self.assign_tensor * embedding_mask\n",
        "        # [batch_size x num_nodes x embedding_dim]\n",
        "        embedding_tensor = self.gcn_forward(x, adj,\n",
        "                self.conv_first, self.conv_block, self.conv_last, embedding_mask)\n",
        "\n",
        "        out, _ = torch.max(embedding_tensor, dim=1)\n",
        "        out_all.append(out)\n",
        "        if self.num_aggs == 2:\n",
        "            out = torch.sum(embedding_tensor, dim=1)\n",
        "            out_all.append(out)\n",
        "\n",
        "        for i in range(self.num_pooling):\n",
        "            if batch_num_nodes is not None and i == 0:\n",
        "                embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
        "            else:\n",
        "                embedding_mask = None\n",
        "\n",
        "            self.assign_tensor = self.gcn_forward(x_a, adj, \n",
        "                    self.assign_conv_first_modules[i], self.assign_conv_block_modules[i], self.assign_conv_last_modules[i],\n",
        "                    embedding_mask)\n",
        "            # [batch_size x num_nodes x next_lvl_num_nodes]\n",
        "            self.assign_tensor = nn.Softmax(dim=-1)(self.assign_pred_modules[i](self.assign_tensor))\n",
        "            if embedding_mask is not None:\n",
        "                self.assign_tensor = self.assign_tensor * embedding_mask\n",
        "\n",
        "            # update pooled features and adj matrix\n",
        "            x = torch.matmul(torch.transpose(self.assign_tensor, 1, 2), embedding_tensor)\n",
        "            adj = torch.transpose(self.assign_tensor, 1, 2) @ adj @ self.assign_tensor\n",
        "            x_a = x\n",
        "        \n",
        "            embedding_tensor = self.gcn_forward(x, adj, \n",
        "                    self.conv_first_after_pool[i], self.conv_block_after_pool[i],\n",
        "                    self.conv_last_after_pool[i])\n",
        "\n",
        "\n",
        "            out, _ = torch.max(embedding_tensor, dim=1)\n",
        "            out_all.append(out)\n",
        "            if self.num_aggs == 2:\n",
        "                #out = torch.mean(embedding_tensor, dim=1)\n",
        "                out = torch.sum(embedding_tensor, dim=1)\n",
        "                out_all.append(out)\n",
        "\n",
        "\n",
        "        if self.concat:\n",
        "            output = torch.cat(out_all, dim=1)\n",
        "        else:\n",
        "            output = out\n",
        "        ypred = self.pred_model(output)\n",
        "        return ypred\n",
        "\n",
        "    def loss(self, pred, label, adj=None, batch_num_nodes=None, adj_hop=1):\n",
        "        ''' \n",
        "        Args:\n",
        "            batch_num_nodes: numpy array of number of nodes in each graph in the minibatch.\n",
        "        '''\n",
        "        eps = 1e-7\n",
        "        loss = super(SoftPoolingGcnEncoder, self).loss(pred, label)\n",
        "        if self.linkpred:\n",
        "            max_num_nodes = adj.size()[1]\n",
        "            pred_adj0 = self.assign_tensor @ torch.transpose(self.assign_tensor, 1, 2) \n",
        "            tmp = pred_adj0\n",
        "            pred_adj = pred_adj0\n",
        "            for adj_pow in range(adj_hop-1):\n",
        "                tmp = tmp @ pred_adj0\n",
        "                pred_adj = pred_adj + tmp\n",
        "            pred_adj = torch.min(pred_adj, torch.ones(1, dtype=pred_adj.dtype).cuda())\n",
        "            #print('adj1', torch.sum(pred_adj0) / torch.numel(pred_adj0))\n",
        "            #print('adj2', torch.sum(pred_adj) / torch.numel(pred_adj))\n",
        "            #self.link_loss = F.nll_loss(torch.log(pred_adj), adj)\n",
        "            self.link_loss = -adj * torch.log(pred_adj+eps) - (1-adj) * torch.log(1-pred_adj+eps)\n",
        "            if batch_num_nodes is None:\n",
        "                num_entries = max_num_nodes * max_num_nodes * adj.size()[0]\n",
        "                print('Warning: calculating link pred loss without masking')\n",
        "            else:\n",
        "                num_entries = np.sum(batch_num_nodes * batch_num_nodes)\n",
        "                embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
        "                adj_mask = embedding_mask @ torch.transpose(embedding_mask, 1, 2)\n",
        "                self.link_loss[(1-adj_mask).bool()] = 0.0\n",
        "\n",
        "            self.link_loss = torch.sum(self.link_loss) / float(num_entries)\n",
        "            #print('linkloss: ', self.link_loss)\n",
        "            return loss + self.link_loss\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJi5k2uQXFGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx #graphsampler.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "import util\n",
        "\n",
        "class GraphSampler(torch.utils.data.Dataset):\n",
        "    ''' Sample graphs and nodes in graph\n",
        "    '''\n",
        "    def __init__(self, G_list, features='default', normalize=True, assign_feat='default', max_num_nodes=0):\n",
        "        self.adj_all = []\n",
        "        self.len_all = []\n",
        "        self.feature_all = []\n",
        "        self.label_all = []\n",
        "        \n",
        "        self.assign_feat_all = []\n",
        "\n",
        "        if max_num_nodes == 0:\n",
        "            self.max_num_nodes = max([G.number_of_nodes() for G in G_list])\n",
        "        else:\n",
        "            self.max_num_nodes = max_num_nodes\n",
        "\n",
        "        #if features == 'default':\n",
        "        self.feat_dim = util.node_dict(G_list[0])[0]['feat'].shape[0]\n",
        "\n",
        "        for G in G_list:\n",
        "            adj = np.array(nx.to_numpy_matrix(G))\n",
        "            if normalize:\n",
        "                sqrt_deg = np.diag(1.0 / np.sqrt(np.sum(adj, axis=0, dtype=float).squeeze()))\n",
        "                adj = np.matmul(np.matmul(sqrt_deg, adj), sqrt_deg)\n",
        "            self.adj_all.append(adj)\n",
        "            self.len_all.append(G.number_of_nodes())\n",
        "            self.label_all.append(G.graph['label'])\n",
        "            # feat matrix: max_num_nodes x feat_dim\n",
        "            if features == 'default':\n",
        "                f = np.zeros((self.max_num_nodes, self.feat_dim), dtype=float)\n",
        "                for i,u in enumerate(G.nodes()):\n",
        "                    f[i,:] = util.node_dict(G)[u]['feat']\n",
        "                self.feature_all.append(f)\n",
        "            elif features == 'id':\n",
        "                self.feature_all.append(np.identity(self.max_num_nodes))\n",
        "            elif features == 'deg-num':\n",
        "                degs = np.sum(np.array(adj), 1)\n",
        "                degs = np.expand_dims(np.pad(degs, [0, self.max_num_nodes - G.number_of_nodes()], 0),\n",
        "                                      axis=1)\n",
        "                self.feature_all.append(degs)\n",
        "            elif features == 'deg':\n",
        "                self.max_deg = 10\n",
        "                degs = np.sum(np.array(adj), 1).astype(int)\n",
        "                degs[degs>max_deg] = max_deg\n",
        "                feat = np.zeros((len(degs), self.max_deg + 1))\n",
        "                feat[np.arange(len(degs)), degs] = 1\n",
        "                feat = np.pad(feat, ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                        'constant', constant_values=0)\n",
        "\n",
        "                f = np.zeros((self.max_num_nodes, self.feat_dim), dtype=float)\n",
        "                for i,u in enumerate(util.node_iter(G)):\n",
        "                    f[i,:] = util.node_dict(G)[u]['feat']\n",
        "\n",
        "                feat = np.concatenate((feat, f), axis=1)\n",
        "\n",
        "                self.feature_all.append(feat)\n",
        "            elif features == 'struct':\n",
        "                self.max_deg = 10\n",
        "                degs = np.sum(np.array(adj), 1).astype(int)\n",
        "                degs[degs>10] = 10\n",
        "                feat = np.zeros((len(degs), self.max_deg + 1))\n",
        "                feat[np.arange(len(degs)), degs] = 1\n",
        "                degs = np.pad(feat, ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                        'constant', constant_values=0)\n",
        "\n",
        "                clusterings = np.array(list(nx.clustering(G).values()))\n",
        "                clusterings = np.expand_dims(np.pad(clusterings, \n",
        "                                                    [0, self.max_num_nodes - G.number_of_nodes()],\n",
        "                                                    'constant'),\n",
        "                                             axis=1)\n",
        "                g_feat = np.hstack([degs, clusterings])\n",
        "                if 'feat' in util.node_dict(G)[0]:\n",
        "                    node_feats = np.array([util.node_dict(G)[i]['feat'] for i in range(G.number_of_nodes())])\n",
        "                    node_feats = np.pad(node_feats, ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                                        'constant')\n",
        "                    g_feat = np.hstack([g_feat, node_feats])\n",
        "\n",
        "                self.feature_all.append(g_feat)\n",
        "\n",
        "            if assign_feat == 'id':\n",
        "                self.assign_feat_all.append(\n",
        "                        np.hstack((np.identity(self.max_num_nodes), self.feature_all[-1])) )\n",
        "            else:\n",
        "                self.assign_feat_all.append(self.feature_all[-1])\n",
        "            \n",
        "        self.feat_dim = self.feature_all[0].shape[1]\n",
        "        self.assign_feat_dim = self.assign_feat_all[0].shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.adj_all)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        adj = self.adj_all[idx]\n",
        "        num_nodes = adj.shape[0]\n",
        "        adj_padded = np.zeros((self.max_num_nodes, self.max_num_nodes))\n",
        "        adj_padded[:num_nodes, :num_nodes] = adj\n",
        "\n",
        "        # use all nodes for aggregation (baseline)\n",
        "\n",
        "        return {'adj':adj_padded,\n",
        "                'feats':self.feature_all[idx].copy(),\n",
        "                'label':self.label_all[idx],\n",
        "                'num_nodes': num_nodes,\n",
        "                'assign_feats':self.assign_feat_all[idx].copy()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUURLvCWXGzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #graphsage.py\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SupervisedGraphSage(nn.Module):\n",
        "    ''' GraphSage embeddings\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_classes, enc):\n",
        "        super(SupervisedGraphSage, self).__init__()\n",
        "        self.enc = enc\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(enc.embed_dim, num_classes))\n",
        "        init.xavier_uniform(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.enc(nodes)\n",
        "        scores = embeds.mm(self.weight)\n",
        "        return scores\n",
        "\n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        return self.xent(nn.softmax(scores), labels.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI0hCJuFXPy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx #loaddata.py\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import os\n",
        "import re\n",
        "\n",
        "import util\n",
        "\n",
        "def read_graphfile(datadir, dataname, max_nodes=None):\n",
        "    ''' Read data from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n",
        "        graph index starts with 1 in file\n",
        "    Returns:\n",
        "        List of networkx objects with graph and node labels\n",
        "    '''\n",
        "    prefix = os.path.join(datadir, dataname, dataname)\n",
        "    filename_graph_indic = prefix + '_graph_indicator.txt'\n",
        "    # index of graphs that a given node belongs to\n",
        "    graph_indic={}\n",
        "    with open(filename_graph_indic) as f:\n",
        "        i=1\n",
        "        for line in f:\n",
        "            line=line.strip(\"\\n\")\n",
        "            graph_indic[i]=int(line)\n",
        "            i+=1\n",
        "\n",
        "    filename_nodes=prefix + '_node_labels.txt'\n",
        "    node_labels=[]\n",
        "    try:\n",
        "        with open(filename_nodes) as f:\n",
        "            for line in f:\n",
        "                line=line.strip(\"\\n\")\n",
        "                node_labels+=[int(line) - 1]\n",
        "        num_unique_node_labels = max(node_labels) + 1\n",
        "    except IOError:\n",
        "        print('No node labels')\n",
        " \n",
        "    filename_node_attrs=prefix + '_node_attributes.txt'\n",
        "    node_attrs=[]\n",
        "    try:\n",
        "        with open(filename_node_attrs) as f:\n",
        "            for line in f:\n",
        "                line = line.strip(\"\\s\\n\")\n",
        "                attrs = [float(attr) for attr in re.split(\"[,\\s]+\", line) if not attr == '']\n",
        "                node_attrs.append(np.array(attrs))\n",
        "    except IOError:\n",
        "        print('No node attributes')\n",
        "       \n",
        "    label_has_zero = False\n",
        "    filename_graphs=prefix + '_graph_labels.txt'\n",
        "    graph_labels=[]\n",
        "\n",
        "    # assume that all graph labels appear in the dataset \n",
        "    #(set of labels don't have to be consecutive)\n",
        "    label_vals = []\n",
        "    with open(filename_graphs) as f:\n",
        "        for line in f:\n",
        "            line=line.strip(\"\\n\")\n",
        "            val = int(line)\n",
        "            #if val == 0:\n",
        "            #    label_has_zero = True\n",
        "            if val not in label_vals:\n",
        "                label_vals.append(val)\n",
        "            graph_labels.append(val)\n",
        "    #graph_labels = np.array(graph_labels)\n",
        "    label_map_to_int = {val: i for i, val in enumerate(label_vals)}\n",
        "    graph_labels = np.array([label_map_to_int[l] for l in graph_labels])\n",
        "    #if label_has_zero:\n",
        "    #    graph_labels += 1\n",
        "    \n",
        "    filename_adj=prefix + '_A.txt'\n",
        "    adj_list={i:[] for i in range(1,len(graph_labels)+1)}    \n",
        "    index_graph={i:[] for i in range(1,len(graph_labels)+1)}\n",
        "    num_edges = 0\n",
        "    with open(filename_adj) as f:\n",
        "        for line in f:\n",
        "            line=line.strip(\"\\n\").split(\",\")\n",
        "            e0,e1=(int(line[0].strip(\" \")),int(line[1].strip(\" \")))\n",
        "            adj_list[graph_indic[e0]].append((e0,e1))\n",
        "            index_graph[graph_indic[e0]]+=[e0,e1]\n",
        "            num_edges += 1\n",
        "    for k in index_graph.keys():\n",
        "        index_graph[k]=[u-1 for u in set(index_graph[k])]\n",
        "\n",
        "    graphs=[]\n",
        "    for i in range(1,1+len(adj_list)):\n",
        "        # indexed from 1 here\n",
        "        G=nx.from_edgelist(adj_list[i])\n",
        "        if max_nodes is not None and G.number_of_nodes() > max_nodes:\n",
        "            continue\n",
        "      \n",
        "        # add features and labels\n",
        "        G.graph['label'] = graph_labels[i-1]\n",
        "        for u in util.node_iter(G):\n",
        "            if len(node_labels) > 0:\n",
        "                node_label_one_hot = [0] * num_unique_node_labels\n",
        "                node_label = node_labels[u-1]\n",
        "                node_label_one_hot[node_label] = 1\n",
        "                util.node_dict(G)[u]['label'] = node_label_one_hot\n",
        "            if len(node_attrs) > 0:\n",
        "                util.node_dict(G)[u]['feat'] = node_attrs[u-1]\n",
        "        if len(node_attrs) > 0:\n",
        "            G.graph['feat_dim'] = node_attrs[0].shape[0]\n",
        "\n",
        "        # relabeling\n",
        "        mapping={}\n",
        "        it=0\n",
        "        for n in util.node_iter(G):\n",
        "            mapping[n]=it\n",
        "            it+=1\n",
        "            \n",
        "        # indexed from 0\n",
        "        graphs.append(nx.relabel_nodes(G, mapping))\n",
        "        print (\"Total Graphs - %d ::\"%(len(graphs)))\n",
        "        print (\"Sample 1st graph - \")\n",
        "        nx.draw(graphs[0],with_labels=True)\n",
        "    return graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hGCgW_IXjRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx #partition.py\n",
        "import numpy as np\n",
        "\n",
        "def partition(embeddings):\n",
        "    ''' Compute a partition of embeddings, where each partition is pooled together.\n",
        "    Args:\n",
        "        embeddings: N-by-D matrix, where N is the number of node embeddings, and D\n",
        "            is the embedding dimension.\n",
        "    '''\n",
        "    dist = np.dot(embeddings)\n",
        "    \n",
        "def kruskal(adj):\n",
        "    # initialize MST\n",
        "    MST = set()\n",
        "    edges = set()\n",
        "    num_nodes = adj.shape[0]\n",
        "    # collect all edges from graph G\n",
        "    for j in range(num_nodes):\n",
        "        for k in range(num_nodes):\n",
        "            if G.graph[j][k] != 0 and (k, j) not in edges:\n",
        "                edges.add((j, k))\n",
        "    # sort all edges in graph G by weights from smallest to largest\n",
        "    sorted_edges = sorted(edges, key=lambda e:G.graph[e[0]][e[1]])\n",
        "    uf = UF(G.vertices)\n",
        "    for e in sorted_edges:\n",
        "        u, v = e\n",
        "        # if u, v already connected, abort this edge\n",
        "        if uf.connected(u, v):\n",
        "            continue\n",
        "        # if not, connect them and add this edge to the MST\n",
        "        uf.union(u, v)\n",
        "        MST.add(e)\n",
        "    return MST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbsTWzmhXlc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #set2set.py\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Set2Set(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, act_fn=nn.ReLU, num_layers=1):\n",
        "        '''\n",
        "        Args:\n",
        "            input_dim: input dim of Set2Set. \n",
        "            hidden_dim: the dim of set representation, which is also the INPUT dimension of \n",
        "                the LSTM in Set2Set. \n",
        "                This is a concatenation of weighted sum of embedding (dim input_dim), and the LSTM\n",
        "                hidden/output (dim: self.lstm_output_dim).\n",
        "        '''\n",
        "        super(Set2Set, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        if hidden_dim <= input_dim:\n",
        "            print('ERROR: Set2Set output_dim should be larger than input_dim')\n",
        "        # the hidden is a concatenation of weighted sum of embedding and LSTM output\n",
        "        self.lstm_output_dim = hidden_dim - input_dim\n",
        "        self.lstm = nn.LSTM(hidden_dim, input_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        # convert back to dim of input_dim\n",
        "        self.pred = nn.Linear(hidden_dim, input_dim)\n",
        "        self.act = act_fn()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        '''\n",
        "        Args:\n",
        "            embedding: [batch_size x n x d] embedding matrix\n",
        "        Returns:\n",
        "            aggregated: [batch_size x d] vector representation of all embeddings\n",
        "        '''\n",
        "        batch_size = embedding.size()[0]\n",
        "        n = embedding.size()[1]\n",
        "\n",
        "        hidden = (torch.zeros(self.num_layers, batch_size, self.lstm_output_dim).cuda(),\n",
        "                  torch.zeros(self.num_layers, batch_size, self.lstm_output_dim).cuda())\n",
        "\n",
        "        q_star = torch.zeros(batch_size, 1, self.hidden_dim).cuda()\n",
        "        for i in range(n):\n",
        "            # q: batch_size x 1 x input_dim\n",
        "            q, hidden = self.lstm(q_star, hidden)\n",
        "            # e: batch_size x n x 1\n",
        "            e = embedding @ torch.transpose(q, 1, 2)\n",
        "            a = nn.Softmax(dim=1)(e)\n",
        "            r = torch.sum(a * embedding, dim=1, keepdim=True)\n",
        "            q_star = torch.cat((q, r), dim=2)\n",
        "        q_star = torch.squeeze(q_star, dim=1)\n",
        "        out = self.act(self.pred(q_star))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbRvI1f_YfiS",
        "colab_type": "code",
        "outputId": "6a18a759-b08d-4685-ae9a-37617448553b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.1.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl4yosuqXu4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib #train.py\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import tensorboardX\n",
        "from tensorboardX import SummaryWriter\n",
        "%matplotlib inline\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import cross_val\n",
        "import encoders\n",
        "import gen.feat as featgen\n",
        "import gen.data as datagen\n",
        "from graph_sampler import GraphSampler\n",
        "import load_data\n",
        "import util\n",
        "\n",
        "\n",
        "def evaluate(dataset, model, args, name='Validation', max_num_examples=None):\n",
        "    model.eval()\n",
        "\n",
        "    labels = []\n",
        "    preds = []\n",
        "    for batch_idx, data in enumerate(dataset):\n",
        "        adj = Variable(data['adj'].float(), requires_grad=False).cuda()\n",
        "        h0 = Variable(data['feats'].float()).cuda()\n",
        "        labels.append(data['label'].long().numpy())\n",
        "        batch_num_nodes = data['num_nodes'].int().numpy()\n",
        "        assign_input = Variable(data['assign_feats'].float(), requires_grad=False).cuda()\n",
        "\n",
        "        ypred = model(h0, adj, batch_num_nodes, assign_x=assign_input)\n",
        "        _, indices = torch.max(ypred, 1)\n",
        "        preds.append(indices.cpu().data.numpy())\n",
        "\n",
        "        if max_num_examples is not None:\n",
        "            if (batch_idx+1)*args.batch_size > max_num_examples:\n",
        "                break\n",
        "\n",
        "    labels = np.hstack(labels)\n",
        "    preds = np.hstack(preds)\n",
        "    \n",
        "    result = {'prec': metrics.precision_score(labels, preds, average='macro'),\n",
        "              'recall': metrics.recall_score(labels, preds, average='macro'),\n",
        "              'acc': metrics.accuracy_score(labels, preds),\n",
        "              'F1': metrics.f1_score(labels, preds, average=\"micro\")}\n",
        "    print(name, \" accuracy:\", result['acc'])\n",
        "    return result\n",
        "\n",
        "def gen_prefix(args):\n",
        "    if args.bmname is not None:\n",
        "        name = args.bmname\n",
        "    else:\n",
        "        name = args.dataset\n",
        "    name += '_' + args.method\n",
        "    if args.method == 'soft-assign':\n",
        "        name += '_l' + str(args.num_gc_layers) + 'x' + str(args.num_pool)\n",
        "        name += '_ar' + str(int(args.assign_ratio*100))\n",
        "        if args.linkpred:\n",
        "            name += '_lp'\n",
        "    else:\n",
        "        name += '_l' + str(args.num_gc_layers)\n",
        "    name += '_h' + str(args.hidden_dim) + '_o' + str(args.output_dim)\n",
        "    if not args.bias:\n",
        "        name += '_nobias'\n",
        "    if len(args.name_suffix) > 0:\n",
        "        name += '_' + args.name_suffix\n",
        "    return name\n",
        "\n",
        "def gen_train_plt_name(args):\n",
        "    return 'results/' + gen_prefix(args) + '.png'\n",
        "\n",
        "def log_assignment(assign_tensor, writer, epoch, batch_idx):\n",
        "    plt.switch_backend('agg')\n",
        "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
        "\n",
        "    # has to be smaller than args.batch_size\n",
        "    for i in range(len(batch_idx)):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(assign_tensor.cpu().data.numpy()[batch_idx[i]], cmap=plt.get_cmap('BuPu'))\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.solids.set_edgecolor(\"face\")\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    #data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "    #data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    data = tensorboardX.utils.figure_to_image(fig)\n",
        "    writer.add_image('assignment', data, epoch)\n",
        "\n",
        "def log_graph(adj, batch_num_nodes, writer, epoch, batch_idx, assign_tensor=None):\n",
        "    plt.switch_backend('agg')\n",
        "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
        "\n",
        "    for i in range(len(batch_idx)):\n",
        "        ax = plt.subplot(2, 2, i+1)\n",
        "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "        G = nx.from_numpy_matrix(adj_matrix)\n",
        "        nx.draw(G, pos=nx.spring_layout(G), with_labels=True, node_color='#336699',\n",
        "                edge_color='grey', width=0.5, node_size=300,\n",
        "                alpha=0.7)\n",
        "        ax.xaxis.set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    data = tensorboardX.utils.figure_to_image(fig)\n",
        "    writer.add_image('graphs', data, epoch)\n",
        "\n",
        "    # colored according to assignment\n",
        "    assignment = assign_tensor.cpu().data.numpy()\n",
        "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
        "\n",
        "    num_clusters = assignment.shape[2]\n",
        "    all_colors = np.array(range(num_clusters))\n",
        "\n",
        "    for i in range(len(batch_idx)):\n",
        "        ax = plt.subplot(2, 2, i+1)\n",
        "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "\n",
        "        label = np.argmax(assignment[batch_idx[i]], axis=1).astype(int)\n",
        "        label = label[: batch_num_nodes[batch_idx[i]]]\n",
        "        node_colors = all_colors[label]\n",
        "\n",
        "        G = nx.from_numpy_matrix(adj_matrix)\n",
        "        nx.draw(G, pos=nx.spring_layout(G), with_labels=False, node_color=node_colors,\n",
        "                edge_color='grey', width=0.4, node_size=50, cmap=plt.get_cmap('Set1'),\n",
        "                vmin=0, vmax=num_clusters-1,\n",
        "                alpha=0.8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    data = tensorboardX.utils.figure_to_image(fig)\n",
        "    writer.add_image('graphs_colored', data, epoch)\n",
        "\n",
        "\n",
        "def train(dataset, model, args, same_feat=True, val_dataset=None, test_dataset=None, writer=None,\n",
        "        mask_nodes = True):\n",
        "    writer_batch_idx = [0, 3, 6, 9]\n",
        "    \n",
        "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=0.001)\n",
        "    iter = 0\n",
        "    best_val_result = {\n",
        "            'epoch': 0,\n",
        "            'loss': 0,\n",
        "            'acc': 0}\n",
        "    test_result = {\n",
        "            'epoch': 0,\n",
        "            'loss': 0,\n",
        "            'acc': 0}\n",
        "    train_accs = []\n",
        "    train_epochs = []\n",
        "    best_val_accs = []\n",
        "    best_val_epochs = []\n",
        "    test_accs = []\n",
        "    test_epochs = []\n",
        "    val_accs = []\n",
        "    for epoch in range(args.num_epochs):\n",
        "        total_time = 0\n",
        "        avg_loss = 0.0\n",
        "        model.train()\n",
        "        print('Epoch: ', epoch)\n",
        "        for batch_idx, data in enumerate(dataset):\n",
        "            begin_time = time.time()\n",
        "            model.zero_grad()\n",
        "            adj = Variable(data['adj'].float(), requires_grad=False).cuda()\n",
        "            h0 = Variable(data['feats'].float(), requires_grad=False).cuda()\n",
        "            label = Variable(data['label'].long()).cuda()\n",
        "            batch_num_nodes = data['num_nodes'].int().numpy() if mask_nodes else None\n",
        "            assign_input = Variable(data['assign_feats'].float(), requires_grad=False).cuda()\n",
        "\n",
        "            ypred = model(h0, adj, batch_num_nodes, assign_x=assign_input)\n",
        "            if not args.method == 'soft-assign' or not args.linkpred:\n",
        "                loss = model.loss(ypred, label)\n",
        "            else:\n",
        "                loss = model.loss(ypred, label, adj, batch_num_nodes)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "            optimizer.step()\n",
        "            iter += 1\n",
        "            avg_loss += loss\n",
        "           \n",
        "            elapsed = time.time() - begin_time\n",
        "            total_time += elapsed\n",
        "\n",
        "            # log once per XX epochs\n",
        "            if epoch % 10 == 0 and batch_idx == len(dataset) // 2 and args.method == 'soft-assign' and writer is not None:\n",
        "                log_assignment(model.assign_tensor, writer, epoch, writer_batch_idx)\n",
        "                if args.log_graph:\n",
        "                    log_graph(adj, batch_num_nodes, writer, epoch, writer_batch_idx, model.assign_tensor)\n",
        "        avg_loss /= batch_idx + 1\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('loss/avg_loss', avg_loss, epoch)\n",
        "            if args.linkpred:\n",
        "                writer.add_scalar('loss/linkpred_loss', model.link_loss, epoch)\n",
        "        print('Avg loss: ', avg_loss, '; epoch time: ', total_time)\n",
        "        result = evaluate(dataset, model, args, name='Train', max_num_examples=100)\n",
        "        train_accs.append(result['acc'])\n",
        "        train_epochs.append(epoch)\n",
        "        if val_dataset is not None:\n",
        "            val_result = evaluate(val_dataset, model, args, name='Validation')\n",
        "            val_accs.append(val_result['acc'])\n",
        "        if val_result['acc'] > best_val_result['acc'] - 1e-7:\n",
        "            best_val_result['acc'] = val_result['acc']\n",
        "            best_val_result['epoch'] = epoch\n",
        "            best_val_result['loss'] = avg_loss\n",
        "        if test_dataset is not None:\n",
        "            test_result = evaluate(test_dataset, model, args, name='Test')\n",
        "            test_result['epoch'] = epoch\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('acc/train_acc', result['acc'], epoch)\n",
        "            writer.add_scalar('acc/val_acc', val_result['acc'], epoch)\n",
        "            writer.add_scalar('loss/best_val_loss', best_val_result['loss'], epoch)\n",
        "            if test_dataset is not None:\n",
        "                writer.add_scalar('acc/test_acc', test_result['acc'], epoch)\n",
        "\n",
        "        print('Best val result: ', best_val_result)\n",
        "        best_val_epochs.append(best_val_result['epoch'])\n",
        "        best_val_accs.append(best_val_result['acc'])\n",
        "        if test_dataset is not None:\n",
        "            print('Test result: ', test_result)\n",
        "            test_epochs.append(test_result['epoch'])\n",
        "            test_accs.append(test_result['acc'])\n",
        "\n",
        "    matplotlib.style.use('seaborn')\n",
        "    print (1)\n",
        "    #plt.switch_backend('agg')   ##################################################\n",
        "    print (2)\n",
        "    plt.plot(train_epochs, util.exp_moving_avg(train_accs, 0.85), '-', lw=1)\n",
        "    plt.show()\n",
        "    print (3)\n",
        "    if test_dataset is not None:\n",
        "        print (4)\n",
        "        plt.plot(best_val_epochs, best_val_accs, 'bo', test_epochs, test_accs, 'go')\n",
        "        plt.legend(['train', 'val', 'test'])\n",
        "    else:\n",
        "        print (5)\n",
        "        plt.plot(best_val_epochs, best_val_accs, 'bo')\n",
        "        plt.legend(['train', 'val'])\n",
        "    #plt.savefig(gen_train_plt_name(args), dpi=600)  ##################################################\n",
        "    print (6)\n",
        "    #plt.close()              ##################################################\n",
        "    print (7)\n",
        "    matplotlib.style.use('default')\n",
        "    print (8)\n",
        "    return model, val_accs\n",
        "\n",
        "def prepare_data(graphs, args, test_graphs=None, max_nodes=0):\n",
        "\n",
        "    random.shuffle(graphs)\n",
        "    if test_graphs is None:\n",
        "        train_idx = int(len(graphs) * args.train_ratio)\n",
        "        test_idx = int(len(graphs) * (1-args.test_ratio))\n",
        "        train_graphs = graphs[:train_idx]\n",
        "        val_graphs = graphs[train_idx: test_idx]\n",
        "        test_graphs = graphs[test_idx:]\n",
        "    else:\n",
        "        train_idx = int(len(graphs) * args.train_ratio)\n",
        "        train_graphs = graphs[:train_idx]\n",
        "        val_graphs = graphs[train_idx:]\n",
        "    print('Num training graphs: ', len(train_graphs), \n",
        "          '; Num validation graphs: ', len(val_graphs),\n",
        "          '; Num testing graphs: ', len(test_graphs))\n",
        "\n",
        "    print('Number of graphs: ', len(graphs))\n",
        "    print('Number of edges: ', sum([G.number_of_edges() for G in graphs]))\n",
        "    print('Max, avg, std of graph size: ', \n",
        "            max([G.number_of_nodes() for G in graphs]), ', '\n",
        "            \"{0:.2f}\".format(np.mean([G.number_of_nodes() for G in graphs])), ', '\n",
        "            \"{0:.2f}\".format(np.std([G.number_of_nodes() for G in graphs])))\n",
        "\n",
        "    # minibatch\n",
        "    dataset_sampler = GraphSampler(train_graphs, normalize=False, max_num_nodes=max_nodes,\n",
        "            features=args.feature_type)\n",
        "    train_dataset_loader = torch.utils.data.DataLoader(\n",
        "            dataset_sampler, \n",
        "            batch_size=args.batch_size, \n",
        "            shuffle=True,\n",
        "            num_workers=args.num_workers)\n",
        "\n",
        "    dataset_sampler = GraphSampler(val_graphs, normalize=False, max_num_nodes=max_nodes,\n",
        "            features=args.feature_type)\n",
        "    val_dataset_loader = torch.utils.data.DataLoader(\n",
        "            dataset_sampler, \n",
        "            batch_size=args.batch_size, \n",
        "            shuffle=False,\n",
        "            num_workers=args.num_workers)\n",
        "\n",
        "    dataset_sampler = GraphSampler(test_graphs, normalize=False, max_num_nodes=max_nodes,\n",
        "            features=args.feature_type)\n",
        "    test_dataset_loader = torch.utils.data.DataLoader(\n",
        "            dataset_sampler, \n",
        "            batch_size=args.batch_size, \n",
        "            shuffle=False,\n",
        "            num_workers=args.num_workers)\n",
        "\n",
        "    print (\"Successfully prepared data\")\n",
        "    return train_dataset_loader, val_dataset_loader, test_dataset_loader, \\\n",
        "            dataset_sampler.max_num_nodes, dataset_sampler.feat_dim, dataset_sampler.assign_feat_dim\n",
        "\n",
        "def syn_community1v2(args, writer=None, export_graphs=False):\n",
        "\n",
        "    # data\n",
        "    graphs1 = datagen.gen_ba(range(40, 60), range(4, 5), 500, \n",
        "            featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))) #This is a list of 500 random graphs with label 0\n",
        "    \n",
        "    for G in graphs1:\n",
        "        G.graph['label'] = 0\n",
        "    if export_graphs:\n",
        "        util.draw_graph_list(graphs1[:16], 4, 4, 'figs/ba')\n",
        "\n",
        "    graphs2 = datagen.gen_2community_ba(range(20, 30), range(4, 5), 500, 0.3, \n",
        "            [featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))]) #This is a list of 500 random graphs with label 1\n",
        "    for G in graphs2:\n",
        "        G.graph['label'] = 1\n",
        "    if export_graphs:\n",
        "        util.draw_graph_list(graphs2[:16], 4, 4, 'figs/ba2')\n",
        "\n",
        "    graphs = graphs1 + graphs2 #list of 1000 graphs\n",
        "    \n",
        "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = prepare_data(graphs, args)\n",
        "    if args.method == 'soft-assign':\n",
        "        print('Method: soft-assign')\n",
        "        model = encoders.SoftPoolingGcnEncoder(\n",
        "                max_num_nodes, \n",
        "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
        "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
        "                bn=args.bn, linkpred=args.linkpred, assign_input_dim=assign_input_dim).cuda()\n",
        "    elif args.method == 'base-set2set':\n",
        "        print('Method: base-set2set')\n",
        "        model = encoders.GcnSet2SetEncoder(input_dim, args.hidden_dim, args.output_dim, 2,\n",
        "                args.num_gc_layers, bn=args.bn).cuda()\n",
        "    else:\n",
        "        print('Method: base')\n",
        "        model = encoders.GcnEncoderGraph(input_dim, args.hidden_dim, args.output_dim, 2,\n",
        "                args.num_gc_layers, bn=args.bn).cuda()\n",
        "\n",
        "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,writer=writer)\n",
        "\n",
        "def syn_community2hier(args, writer=None):\n",
        "\n",
        "    # data\n",
        "    feat_gen = [featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))]\n",
        "    graphs1 = datagen.gen_2hier(1000, [2,4], 10, range(4,5), 0.1, 0.03, feat_gen)\n",
        "    graphs2 = datagen.gen_2hier(1000, [3,3], 10, range(4,5), 0.1, 0.03, feat_gen)\n",
        "    graphs3 = datagen.gen_2community_ba(range(28, 33), range(4,7), 1000, 0.25, feat_gen)\n",
        "\n",
        "    for G in graphs1:\n",
        "        G.graph['label'] = 0\n",
        "    for G in graphs2:\n",
        "        G.graph['label'] = 1\n",
        "    for G in graphs3:\n",
        "        G.graph['label'] = 2\n",
        "\n",
        "    graphs = graphs1 + graphs2 + graphs3\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = prepare_data(graphs, args)\n",
        "\n",
        "    if args.method == 'soft-assign':\n",
        "        print('Method: soft-assign')\n",
        "        model = encoders.SoftPoolingGcnEncoder(\n",
        "                max_num_nodes, \n",
        "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
        "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
        "                bn=args.bn, linkpred=args.linkpred, args=args, assign_input_dim=assign_input_dim).cuda()\n",
        "    elif args.method == 'base-set2set':\n",
        "        print('Method: base-set2set')\n",
        "        model = encoders.GcnSet2SetEncoder(input_dim, args.hidden_dim, args.output_dim, 2,\n",
        "                args.num_gc_layers, bn=args.bn, args=args, assign_input_dim=assign_input_dim).cuda()\n",
        "    else:\n",
        "        print('Method: base')\n",
        "        model = encoders.GcnEncoderGraph(input_dim, args.hidden_dim, args.output_dim, 2,\n",
        "                args.num_gc_layers, bn=args.bn, args=args).cuda()\n",
        "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,\n",
        "            writer=writer)\n",
        "\n",
        "\n",
        "def pkl_task(args, feat=None):\n",
        "    with open(os.path.join(args.datadir, args.pkl_fname), 'rb') as pkl_file:\n",
        "        data = pickle.load(pkl_file)\n",
        "    graphs = data[0]\n",
        "    labels = data[1]\n",
        "    test_graphs = data[2]\n",
        "    test_labels = data[3]\n",
        "\n",
        "    for i in range(len(graphs)):\n",
        "        graphs[i].graph['label'] = labels[i]\n",
        "    for i in range(len(test_graphs)):\n",
        "        test_graphs[i].graph['label'] = test_labels[i]\n",
        "\n",
        "    if feat is None:\n",
        "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
        "        for G in graphs:\n",
        "            featgen_const.gen_node_features(G)\n",
        "        for G in test_graphs:\n",
        "            featgen_const.gen_node_features(G)\n",
        "\n",
        "    train_dataset, test_dataset, max_num_nodes = prepare_data(graphs, args, test_graphs=test_graphs)\n",
        "    model = encoders.GcnEncoderGraph(\n",
        "            args.input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
        "            args.num_gc_layers, bn=args.bn).cuda()\n",
        "    train(train_dataset, model, args, test_dataset=test_dataset)\n",
        "    evaluate(test_dataset, model, args, 'Validation')\n",
        "\n",
        "def benchmark_task(args, writer=None, feat='node-label'):\n",
        "    graphs = load_data.read_graphfile(args.datadir, args.bmname, max_nodes=args.max_nodes)\n",
        "    \n",
        "    if feat == 'node-feat' and 'feat_dim' in graphs[0].graph:\n",
        "        print('Using node features')\n",
        "        input_dim = graphs[0].graph['feat_dim']\n",
        "    elif feat == 'node-label' and 'label' in graphs[0].node[0]:\n",
        "        print('Using node labels')\n",
        "        for G in graphs:\n",
        "            for u in G.nodes():\n",
        "                G.node[u]['feat'] = np.array(G.node[u]['label'])\n",
        "    else:\n",
        "        print('Using constant labels')\n",
        "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
        "        for G in graphs:\n",
        "            featgen_const.gen_node_features(G)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = \\\n",
        "            prepare_data(graphs, args, max_nodes=args.max_nodes)\n",
        "    if args.method == 'soft-assign':\n",
        "        print('Method: soft-assign')\n",
        "        model = encoders.SoftPoolingGcnEncoder(\n",
        "                max_num_nodes, \n",
        "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
        "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
        "                bn=args.bn, dropout=args.dropout, linkpred=args.linkpred, args=args,\n",
        "                assign_input_dim=assign_input_dim).cuda()\n",
        "    elif args.method == 'base-set2set':\n",
        "        print('Method: base-set2set')\n",
        "        model = encoders.GcnSet2SetEncoder(\n",
        "                input_dim, args.hidden_dim, args.output_dim, args.num_classes,\n",
        "                args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
        "    else:\n",
        "        print('Method: base')\n",
        "        model = encoders.GcnEncoderGraph(\n",
        "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
        "                args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
        "\n",
        "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,\n",
        "            writer=writer)\n",
        "    evaluate(test_dataset, model, args, 'Validation')\n",
        "\n",
        "\n",
        "def benchmark_task_val(args, writer=None, feat='node-label'):\n",
        "    all_vals = []\n",
        "    graphs = load_data.read_graphfile(args.datadir, args.bmname, max_nodes=args.max_nodes)\n",
        "\n",
        "    example_node = util.node_dict(graphs[0])[0]\n",
        "    \n",
        "    if feat == 'node-feat' and 'feat_dim' in graphs[0].graph:\n",
        "        print('Using node features')\n",
        "        input_dim = graphs[0].graph['feat_dim']\n",
        "    elif feat == 'node-label' and 'label' in example_node:\n",
        "        print('Using node labels')\n",
        "        for G in graphs:\n",
        "            for u in G.nodes():\n",
        "                util.node_dict(G)[u]['feat'] = np.array(util.node_dict(G)[u]['label'])\n",
        "    else:\n",
        "        print('Using constant labels')\n",
        "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
        "        for G in graphs:\n",
        "            featgen_const.gen_node_features(G)\n",
        "\n",
        "    for i in range(10):\n",
        "        train_dataset, val_dataset, max_num_nodes, input_dim, assign_input_dim = \\\n",
        "                cross_val.prepare_val_data(graphs, args, i, max_nodes=args.max_nodes)\n",
        "        if args.method == 'soft-assign':\n",
        "            print('Method: soft-assign')\n",
        "            model = encoders.SoftPoolingGcnEncoder(\n",
        "                    max_num_nodes, \n",
        "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
        "                    args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
        "                    bn=args.bn, dropout=args.dropout, linkpred=args.linkpred, args=args,\n",
        "                    assign_input_dim=assign_input_dim).cuda()\n",
        "        elif args.method == 'base-set2set':\n",
        "            print('Method: base-set2set')\n",
        "            model = encoders.GcnSet2SetEncoder(\n",
        "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes,\n",
        "                    args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
        "        else:\n",
        "            print('Method: base')\n",
        "            print (\"Input_Dim - %d :: Hidden_dim - %d :: Output_dim - %d :: Classes_num - %d :: GC_layers - %d\"%(input_dim,args.hidden_dim,args.output_dim,args.num_classes,args.num_gc_layers))\n",
        "            model = encoders.GcnEncoderGraph(\n",
        "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
        "                    args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
        "\n",
        "        _, val_accs = train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=None,\n",
        "            writer=writer)\n",
        "        all_vals.append(np.array(val_accs))\n",
        "    all_vals = np.vstack(all_vals)\n",
        "    all_vals = np.mean(all_vals, axis=0)\n",
        "    print(all_vals)\n",
        "    print(np.max(all_vals))\n",
        "    print(np.argmax(all_vals))\n",
        "    \n",
        "def arg_parse():\n",
        "    parser = argparse.ArgumentParser(description='GraphPool arguments.')\n",
        "    io_parser = parser.add_mutually_exclusive_group(required=False)\n",
        "    io_parser.add_argument('--dataset', dest='dataset', \n",
        "            help='Input dataset.')\n",
        "    benchmark_parser = io_parser.add_argument_group()\n",
        "    benchmark_parser.add_argument('--bmname', dest='bmname',\n",
        "            help='Name of the benchmark dataset')\n",
        "    io_parser.add_argument('--pkl', dest='pkl_fname',\n",
        "            help='Name of the pkl data file')\n",
        "\n",
        "    softpool_parser = parser.add_argument_group()\n",
        "    softpool_parser.add_argument('--assign-ratio', dest='assign_ratio', type=float,\n",
        "            help='ratio of number of nodes in consecutive layers')\n",
        "    softpool_parser.add_argument('--num-pool', dest='num_pool', type=int,\n",
        "            help='number of pooling layers')\n",
        "    parser.add_argument('--linkpred', dest='linkpred', action='store_const',\n",
        "            const=True, default=False,\n",
        "            help='Whether link prediction side objective is used')\n",
        "\n",
        "\n",
        "    parser.add_argument('--datadir', dest='datadir',\n",
        "            help='Directory where benchmark is located')\n",
        "    parser.add_argument('--logdir', dest='logdir',\n",
        "            help='Tensorboard log directory')\n",
        "    parser.add_argument('--cuda', dest='cuda',\n",
        "            help='CUDA.')\n",
        "    parser.add_argument('--max-nodes', dest='max_nodes', type=int,\n",
        "            help='Maximum number of nodes (ignore graghs with nodes exceeding the number.')\n",
        "    parser.add_argument('--lr', dest='lr', type=float,\n",
        "            help='Learning rate.')\n",
        "    parser.add_argument('--clip', dest='clip', type=float,\n",
        "            help='Gradient clipping.')\n",
        "    parser.add_argument('--batch-size', dest='batch_size', type=int,\n",
        "            help='Batch size.')\n",
        "    parser.add_argument('--epochs', dest='num_epochs', type=int,\n",
        "            help='Number of epochs to train.')\n",
        "    parser.add_argument('--train-ratio', dest='train_ratio', type=float,\n",
        "            help='Ratio of number of graphs training set to all graphs.')\n",
        "    parser.add_argument('--num_workers', dest='num_workers', type=int,\n",
        "            help='Number of workers to load data.')\n",
        "    parser.add_argument('--feature', dest='feature_type',\n",
        "            help='Feature used for encoder. Can be: id, deg')\n",
        "    parser.add_argument('--input-dim', dest='input_dim', type=int,\n",
        "            help='Input feature dimension')\n",
        "    parser.add_argument('--hidden-dim', dest='hidden_dim', type=int,\n",
        "            help='Hidden dimension')\n",
        "    parser.add_argument('--output-dim', dest='output_dim', type=int,\n",
        "            help='Output dimension')\n",
        "    parser.add_argument('--num-classes', dest='num_classes', type=int,\n",
        "            help='Number of label classes')\n",
        "    parser.add_argument('--num-gc-layers', dest='num_gc_layers', type=int,\n",
        "            help='Number of graph convolution layers before each pooling')\n",
        "    parser.add_argument('--nobn', dest='bn', action='store_const',\n",
        "            const=False, default=True,\n",
        "            help='Whether batch normalization is used')\n",
        "    parser.add_argument('--dropout', dest='dropout', type=float,\n",
        "            help='Dropout rate.')\n",
        "    parser.add_argument('--nobias', dest='bias', action='store_const',\n",
        "            const=False, default=True,\n",
        "            help='Whether to add bias. Default to True.')\n",
        "    parser.add_argument('--no-log-graph', dest='log_graph', action='store_const',\n",
        "            const=False, default=True,\n",
        "            help='Whether disable log graph')\n",
        "\n",
        "    parser.add_argument('--method', dest='method',\n",
        "            help='Method. Possible values: base, base-set2set, soft-assign')\n",
        "    parser.add_argument('--name-suffix', dest='name_suffix',\n",
        "            help='suffix added to the output filename')\n",
        "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "    # ENZYMES - Diffpool\n",
        "    # python -m train --bmname=ENZYMES --assign-ratio=0.1 --hidden-dim=30 --output-dim=30 --cuda=1 --num-classes=6 --method=soft-assign\n",
        "    # DD - Diffpool\n",
        "    # python -m train --bmname=ENZYMES --assign-ratio=0.1 --hidden-dim=64 --output-dim=64 --cuda=1 --num-classes=2 --method=soft-assign\n",
        "#################################################################################################################################################\n",
        "    # Enzymes dataset\n",
        "# Soft pooling GCN Encoder   \n",
        "\n",
        "    parser.set_defaults(datadir='data',\n",
        "                            bmname='ENZYMES', \n",
        "                            logdir='log',\n",
        "                            max_nodes=1000,\n",
        "                            cuda='1',\n",
        "                            feature_type='default',\n",
        "                            lr=0.001,\n",
        "                            clip=2.0,\n",
        "                            batch_size=20,\n",
        "                            num_epochs=300,\n",
        "                            train_ratio=0.8,\n",
        "                            test_ratio=0.1,\n",
        "                            num_workers=1,\n",
        "                            input_dim=3,\n",
        "                            hidden_dim=30,\n",
        "                            output_dim=30,\n",
        "                            num_classes=6,\n",
        "                            num_gc_layers=3,\n",
        "                            dropout=0.0,\n",
        "                            method='soft-assign',\n",
        "                            name_suffix='',\n",
        "                            assign_ratio=0.1,\n",
        "                            num_pool=2\n",
        "                          )\n",
        "    return parser.parse_args()\n",
        "#################################################################################################################################################\n",
        "\n",
        "def main():\n",
        "    prog_args = arg_parse()\n",
        "\n",
        "    # export scalar data to JSON for external processing\n",
        "    path = os.path.join(prog_args.logdir, gen_prefix(prog_args))\n",
        "    if os.path.isdir(path):\n",
        "        print('Remove existing log dir: ', path)\n",
        "        shutil.rmtree(path)\n",
        "    writer = SummaryWriter(path)\n",
        "    #writer = None\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = prog_args.cuda\n",
        "    print('CUDA', prog_args.cuda)\n",
        "\n",
        "    if prog_args.bmname is not None:\n",
        "        print(\"here\")\n",
        "        benchmark_task_val(prog_args, writer=writer)\n",
        "    elif prog_args.pkl_fname is not None:\n",
        "        print(\"pkl\")\n",
        "        pkl_task(prog_args)\n",
        "    elif prog_args.dataset is not None:\n",
        "        if prog_args.dataset == 'syn1v2':\n",
        "            print (\"syn1v2\")\n",
        "            #syn_community1v2(prog_args, writer=writer)\n",
        "        if prog_args.dataset == 'syn2hier':\n",
        "            syn_community2hier(prog_args, writer=writer)\n",
        "            #print (\"syn2hier\")\n",
        "\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wGrBTR-DW97",
        "colab_type": "code",
        "outputId": "ba9681a9-d28c-4dbe-abd9-f7968267aed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remove existing log dir:  log/ENZYMES_soft-assign_l3x2_ar10_h30_o30\n",
            "CUDA 1\n",
            "here\n",
            "Using node labels\n",
            "Num training graphs:  540 ; Num validation graphs:  60\n",
            "Number of graphs:  600\n",
            "Number of edges:  37282\n",
            "Max, avg, std of graph size:  125 , 32.46 , 14.87\n",
            "Method: soft-assign\n",
            "Epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:71: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:73: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  m.bias.data = init.constant(m.bias.data, 0.0)\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:293: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:295: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  m.bias.data = init.constant(m.bias.data, 0.0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Avg loss:  tensor(1.8013, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8737847805023193\n",
            "Train  accuracy: 0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation  accuracy: 0.23333333333333334\n",
            "Best val result:  {'epoch': 0, 'loss': tensor(1.8013, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.23333333333333334}\n",
            "Epoch:  1\n",
            "Avg loss:  tensor(1.7159, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7341175079345703\n",
            "Train  accuracy: 0.30833333333333335\n",
            "Validation  accuracy: 0.25\n",
            "Best val result:  {'epoch': 1, 'loss': tensor(1.7159, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.25}\n",
            "Epoch:  2\n",
            "Avg loss:  tensor(1.6448, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7101693153381348\n",
            "Train  accuracy: 0.3333333333333333\n",
            "Validation  accuracy: 0.2833333333333333\n",
            "Best val result:  {'epoch': 2, 'loss': tensor(1.6448, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.2833333333333333}\n",
            "Epoch:  3\n",
            "Avg loss:  tensor(1.6034, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.733623743057251\n",
            "Train  accuracy: 0.39166666666666666\n",
            "Validation  accuracy: 0.2833333333333333\n",
            "Best val result:  {'epoch': 3, 'loss': tensor(1.6034, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.2833333333333333}\n",
            "Epoch:  4\n",
            "Avg loss:  tensor(1.5365, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7556214332580566\n",
            "Train  accuracy: 0.39166666666666666\n",
            "Validation  accuracy: 0.23333333333333334\n",
            "Best val result:  {'epoch': 3, 'loss': tensor(1.6034, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.2833333333333333}\n",
            "Epoch:  5\n",
            "Avg loss:  tensor(1.5381, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.849053144454956\n",
            "Train  accuracy: 0.35833333333333334\n",
            "Validation  accuracy: 0.26666666666666666\n",
            "Best val result:  {'epoch': 3, 'loss': tensor(1.6034, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.2833333333333333}\n",
            "Epoch:  6\n",
            "Avg loss:  tensor(1.4890, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7526187896728516\n",
            "Train  accuracy: 0.45\n",
            "Validation  accuracy: 0.21666666666666667\n",
            "Best val result:  {'epoch': 3, 'loss': tensor(1.6034, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.2833333333333333}\n",
            "Epoch:  7\n",
            "Avg loss:  tensor(1.4761, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.788595676422119\n",
            "Train  accuracy: 0.48333333333333334\n",
            "Validation  accuracy: 0.3\n",
            "Best val result:  {'epoch': 7, 'loss': tensor(1.4761, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.3}\n",
            "Epoch:  8\n",
            "Avg loss:  tensor(1.4550, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8993444442749023\n",
            "Train  accuracy: 0.4666666666666667\n",
            "Validation  accuracy: 0.31666666666666665\n",
            "Best val result:  {'epoch': 8, 'loss': tensor(1.4550, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.31666666666666665}\n",
            "Epoch:  9\n",
            "Avg loss:  tensor(1.4272, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7857351303100586\n",
            "Train  accuracy: 0.4083333333333333\n",
            "Validation  accuracy: 0.26666666666666666\n",
            "Best val result:  {'epoch': 8, 'loss': tensor(1.4550, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.31666666666666665}\n",
            "Epoch:  10\n",
            "Avg loss:  tensor(1.4130, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.799701452255249\n",
            "Train  accuracy: 0.5333333333333333\n",
            "Validation  accuracy: 0.23333333333333334\n",
            "Best val result:  {'epoch': 8, 'loss': tensor(1.4550, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.31666666666666665}\n",
            "Epoch:  11\n",
            "Avg loss:  tensor(1.4038, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8156039714813232\n",
            "Train  accuracy: 0.45\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 11, 'loss': tensor(1.4038, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.38333333333333336}\n",
            "Epoch:  12\n",
            "Avg loss:  tensor(1.3802, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7218921184539795\n",
            "Train  accuracy: 0.5583333333333333\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 11, 'loss': tensor(1.4038, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.38333333333333336}\n",
            "Epoch:  13\n",
            "Avg loss:  tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6479740142822266\n",
            "Train  accuracy: 0.48333333333333334\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  14\n",
            "Avg loss:  tensor(1.3116, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8155643939971924\n",
            "Train  accuracy: 0.475\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  15\n",
            "Avg loss:  tensor(1.3137, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.714416265487671\n",
            "Train  accuracy: 0.5\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  16\n",
            "Avg loss:  tensor(1.2909, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8011326789855957\n",
            "Train  accuracy: 0.5083333333333333\n",
            "Validation  accuracy: 0.3\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  17\n",
            "Avg loss:  tensor(1.2637, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.80035138130188\n",
            "Train  accuracy: 0.5416666666666666\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  18\n",
            "Avg loss:  tensor(1.2908, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6883559226989746\n",
            "Train  accuracy: 0.4583333333333333\n",
            "Validation  accuracy: 0.31666666666666665\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  19\n",
            "Avg loss:  tensor(1.2898, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9666688442230225\n",
            "Train  accuracy: 0.5416666666666666\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  20\n",
            "Avg loss:  tensor(1.2138, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7677478790283203\n",
            "Train  accuracy: 0.575\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  21\n",
            "Avg loss:  tensor(1.2337, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7804830074310303\n",
            "Train  accuracy: 0.575\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  22\n",
            "Avg loss:  tensor(1.1797, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.856233835220337\n",
            "Train  accuracy: 0.55\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  23\n",
            "Avg loss:  tensor(1.1916, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7909984588623047\n",
            "Train  accuracy: 0.5583333333333333\n",
            "Validation  accuracy: 0.3\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  24\n",
            "Avg loss:  tensor(1.2104, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.73175048828125\n",
            "Train  accuracy: 0.5\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  25\n",
            "Avg loss:  tensor(1.1665, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8107259273529053\n",
            "Train  accuracy: 0.625\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  26\n",
            "Avg loss:  tensor(1.1286, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7378525733947754\n",
            "Train  accuracy: 0.525\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  27\n",
            "Avg loss:  tensor(1.1104, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.768660545349121\n",
            "Train  accuracy: 0.5583333333333333\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  28\n",
            "Avg loss:  tensor(1.1446, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8145267963409424\n",
            "Train  accuracy: 0.65\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  29\n",
            "Avg loss:  tensor(1.0742, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9107210636138916\n",
            "Train  accuracy: 0.5916666666666667\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  30\n",
            "Avg loss:  tensor(1.0632, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8617238998413086\n",
            "Train  accuracy: 0.6\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  31\n",
            "Avg loss:  tensor(1.0529, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8600518703460693\n",
            "Train  accuracy: 0.575\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  32\n",
            "Avg loss:  tensor(1.0301, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8094449043273926\n",
            "Train  accuracy: 0.7\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  33\n",
            "Avg loss:  tensor(1.0334, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8175125122070312\n",
            "Train  accuracy: 0.5666666666666667\n",
            "Validation  accuracy: 0.3333333333333333\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  34\n",
            "Avg loss:  tensor(1.0236, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6698672771453857\n",
            "Train  accuracy: 0.6666666666666666\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 13, 'loss': tensor(1.3356, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.4}\n",
            "Epoch:  35\n",
            "Avg loss:  tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.689842939376831\n",
            "Train  accuracy: 0.65\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  36\n",
            "Avg loss:  tensor(0.9885, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.821535110473633\n",
            "Train  accuracy: 0.6666666666666666\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  37\n",
            "Avg loss:  tensor(0.9678, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.874183416366577\n",
            "Train  accuracy: 0.5666666666666667\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  38\n",
            "Avg loss:  tensor(0.9702, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8971610069274902\n",
            "Train  accuracy: 0.6833333333333333\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  39\n",
            "Avg loss:  tensor(0.9282, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.726414918899536\n",
            "Train  accuracy: 0.65\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  40\n",
            "Avg loss:  tensor(0.9508, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7767155170440674\n",
            "Train  accuracy: 0.75\n",
            "Validation  accuracy: 0.31666666666666665\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  41\n",
            "Avg loss:  tensor(0.9544, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8847553730010986\n",
            "Train  accuracy: 0.7166666666666667\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  42\n",
            "Avg loss:  tensor(0.9336, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7621958255767822\n",
            "Train  accuracy: 0.6333333333333333\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 35, 'loss': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.43333333333333335}\n",
            "Epoch:  43\n",
            "Avg loss:  tensor(0.9270, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6559054851531982\n",
            "Train  accuracy: 0.75\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 43, 'loss': tensor(0.9270, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  44\n",
            "Avg loss:  tensor(0.9292, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.977415084838867\n",
            "Train  accuracy: 0.7083333333333334\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 43, 'loss': tensor(0.9270, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  45\n",
            "Avg loss:  tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8149640560150146\n",
            "Train  accuracy: 0.65\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 45, 'loss': tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  46\n",
            "Avg loss:  tensor(0.9170, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7633488178253174\n",
            "Train  accuracy: 0.6666666666666666\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 45, 'loss': tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  47\n",
            "Avg loss:  tensor(0.8789, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9546797275543213\n",
            "Train  accuracy: 0.65\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 45, 'loss': tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  48\n",
            "Avg loss:  tensor(0.8900, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8247740268707275\n",
            "Train  accuracy: 0.6916666666666667\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 45, 'loss': tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.45}\n",
            "Epoch:  49\n",
            "Avg loss:  tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.775191068649292\n",
            "Train  accuracy: 0.6666666666666666\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 49, 'loss': tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  50\n",
            "Avg loss:  tensor(0.8115, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8806397914886475\n",
            "Train  accuracy: 0.7083333333333334\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 49, 'loss': tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  51\n",
            "Avg loss:  tensor(0.8663, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8498873710632324\n",
            "Train  accuracy: 0.725\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 49, 'loss': tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  52\n",
            "Avg loss:  tensor(0.7806, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8315353393554688\n",
            "Train  accuracy: 0.7\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 49, 'loss': tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  53\n",
            "Avg loss:  tensor(0.8117, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.841986894607544\n",
            "Train  accuracy: 0.7583333333333333\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 49, 'loss': tensor(0.8503, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  54\n",
            "Avg loss:  tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7226502895355225\n",
            "Train  accuracy: 0.6916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  55\n",
            "Avg loss:  tensor(0.7809, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.773724317550659\n",
            "Train  accuracy: 0.7166666666666667\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  56\n",
            "Avg loss:  tensor(0.7947, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.825047492980957\n",
            "Train  accuracy: 0.75\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  57\n",
            "Avg loss:  tensor(0.7746, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.792495012283325\n",
            "Train  accuracy: 0.775\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  58\n",
            "Avg loss:  tensor(0.7875, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.803785562515259\n",
            "Train  accuracy: 0.7\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  59\n",
            "Avg loss:  tensor(0.7167, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8393452167510986\n",
            "Train  accuracy: 0.7333333333333333\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  60\n",
            "Avg loss:  tensor(0.7430, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8993470668792725\n",
            "Train  accuracy: 0.7416666666666667\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  61\n",
            "Avg loss:  tensor(0.7470, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.775094747543335\n",
            "Train  accuracy: 0.85\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  62\n",
            "Avg loss:  tensor(0.7262, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8382930755615234\n",
            "Train  accuracy: 0.7583333333333333\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  63\n",
            "Avg loss:  tensor(0.6880, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.791990280151367\n",
            "Train  accuracy: 0.7166666666666667\n",
            "Validation  accuracy: 0.35\n",
            "Best val result:  {'epoch': 54, 'loss': tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  64\n",
            "Avg loss:  tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7463064193725586\n",
            "Train  accuracy: 0.7333333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 64, 'loss': tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  65\n",
            "Avg loss:  tensor(0.7039, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.710951805114746\n",
            "Train  accuracy: 0.7916666666666666\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 64, 'loss': tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  66\n",
            "Avg loss:  tensor(0.6253, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.868549108505249\n",
            "Train  accuracy: 0.75\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 64, 'loss': tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  67\n",
            "Avg loss:  tensor(0.6355, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7970449924468994\n",
            "Train  accuracy: 0.7833333333333333\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 64, 'loss': tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  68\n",
            "Avg loss:  tensor(0.6709, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7824020385742188\n",
            "Train  accuracy: 0.7833333333333333\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 64, 'loss': tensor(0.7024, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.48333333333333334}\n",
            "Epoch:  69\n",
            "Avg loss:  tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7611541748046875\n",
            "Train  accuracy: 0.7333333333333333\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  70\n",
            "Avg loss:  tensor(0.6618, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9287643432617188\n",
            "Train  accuracy: 0.8\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  71\n",
            "Avg loss:  tensor(0.6210, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.031664609909058\n",
            "Train  accuracy: 0.825\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  72\n",
            "Avg loss:  tensor(0.5987, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6933815479278564\n",
            "Train  accuracy: 0.8\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  73\n",
            "Avg loss:  tensor(0.6456, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.922553777694702\n",
            "Train  accuracy: 0.8083333333333333\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  74\n",
            "Avg loss:  tensor(0.6245, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7326509952545166\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  75\n",
            "Avg loss:  tensor(0.6342, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.822467088699341\n",
            "Train  accuracy: 0.75\n",
            "Validation  accuracy: 0.36666666666666664\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  76\n",
            "Avg loss:  tensor(0.5447, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.824801445007324\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  77\n",
            "Avg loss:  tensor(0.6120, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7542641162872314\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  78\n",
            "Avg loss:  tensor(0.5435, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8195714950561523\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  79\n",
            "Avg loss:  tensor(0.5480, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9126200675964355\n",
            "Train  accuracy: 0.7833333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  80\n",
            "Avg loss:  tensor(0.5580, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.887800693511963\n",
            "Train  accuracy: 0.8166666666666667\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  81\n",
            "Avg loss:  tensor(0.5196, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7103936672210693\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  82\n",
            "Avg loss:  tensor(0.6060, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.718338966369629\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  83\n",
            "Avg loss:  tensor(0.5267, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8813695907592773\n",
            "Train  accuracy: 0.85\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  84\n",
            "Avg loss:  tensor(0.5248, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8458261489868164\n",
            "Train  accuracy: 0.7916666666666666\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  85\n",
            "Avg loss:  tensor(0.5518, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.703537940979004\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  86\n",
            "Avg loss:  tensor(0.5132, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.843287944793701\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  87\n",
            "Avg loss:  tensor(0.4973, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.751401662826538\n",
            "Train  accuracy: 0.825\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  88\n",
            "Avg loss:  tensor(0.4757, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.770117998123169\n",
            "Train  accuracy: 0.8\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  89\n",
            "Avg loss:  tensor(0.5076, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8422257900238037\n",
            "Train  accuracy: 0.7916666666666666\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  90\n",
            "Avg loss:  tensor(0.5091, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.12997579574585\n",
            "Train  accuracy: 0.825\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  91\n",
            "Avg loss:  tensor(0.4727, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9585447311401367\n",
            "Train  accuracy: 0.8083333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  92\n",
            "Avg loss:  tensor(0.4332, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8673782348632812\n",
            "Train  accuracy: 0.85\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  93\n",
            "Avg loss:  tensor(0.4828, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8072662353515625\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  94\n",
            "Avg loss:  tensor(0.5088, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.757894992828369\n",
            "Train  accuracy: 0.7583333333333333\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  95\n",
            "Avg loss:  tensor(0.4208, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.729905605316162\n",
            "Train  accuracy: 0.8666666666666667\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  96\n",
            "Avg loss:  tensor(0.4590, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8716835975646973\n",
            "Train  accuracy: 0.8833333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  97\n",
            "Avg loss:  tensor(0.4460, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9075074195861816\n",
            "Train  accuracy: 0.9\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  98\n",
            "Avg loss:  tensor(0.4186, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.78023099899292\n",
            "Train  accuracy: 0.8333333333333334\n",
            "Validation  accuracy: 0.38333333333333336\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  99\n",
            "Avg loss:  tensor(0.4217, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.817106246948242\n",
            "Train  accuracy: 0.8666666666666667\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  100\n",
            "Avg loss:  tensor(0.4422, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.747694730758667\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  101\n",
            "Avg loss:  tensor(0.4328, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.847038984298706\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  102\n",
            "Avg loss:  tensor(0.3851, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8701283931732178\n",
            "Train  accuracy: 0.8333333333333334\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  103\n",
            "Avg loss:  tensor(0.4255, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.885101795196533\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  104\n",
            "Avg loss:  tensor(0.4294, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.937281370162964\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  105\n",
            "Avg loss:  tensor(0.4099, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.846571683883667\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  106\n",
            "Avg loss:  tensor(0.3940, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.947283983230591\n",
            "Train  accuracy: 0.8916666666666667\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  107\n",
            "Avg loss:  tensor(0.3653, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8060898780822754\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  108\n",
            "Avg loss:  tensor(0.4285, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7025158405303955\n",
            "Train  accuracy: 0.8416666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  109\n",
            "Avg loss:  tensor(0.3941, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.746514081954956\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  110\n",
            "Avg loss:  tensor(0.3931, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.0909764766693115\n",
            "Train  accuracy: 0.825\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  111\n",
            "Avg loss:  tensor(0.4240, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.887357711791992\n",
            "Train  accuracy: 0.875\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  112\n",
            "Avg loss:  tensor(0.3942, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8830647468566895\n",
            "Train  accuracy: 0.8916666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  113\n",
            "Avg loss:  tensor(0.3560, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8081305027008057\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  114\n",
            "Avg loss:  tensor(0.3857, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6934139728546143\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  115\n",
            "Avg loss:  tensor(0.3536, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.889876365661621\n",
            "Train  accuracy: 0.9\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  116\n",
            "Avg loss:  tensor(0.4105, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8728673458099365\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  117\n",
            "Avg loss:  tensor(0.3676, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.891859292984009\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 69, 'loss': tensor(0.6803, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.55}\n",
            "Epoch:  118\n",
            "Avg loss:  tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.789473533630371\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  119\n",
            "Avg loss:  tensor(0.3494, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9002811908721924\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  120\n",
            "Avg loss:  tensor(0.2903, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8582282066345215\n",
            "Train  accuracy: 0.8333333333333334\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  121\n",
            "Avg loss:  tensor(0.3699, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9032814502716064\n",
            "Train  accuracy: 0.8666666666666667\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  122\n",
            "Avg loss:  tensor(0.3200, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8755745887756348\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  123\n",
            "Avg loss:  tensor(0.2763, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8429925441741943\n",
            "Train  accuracy: 0.8583333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  124\n",
            "Avg loss:  tensor(0.3237, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.781280755996704\n",
            "Train  accuracy: 0.825\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  125\n",
            "Avg loss:  tensor(0.2906, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7383155822753906\n",
            "Train  accuracy: 0.8666666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  126\n",
            "Avg loss:  tensor(0.3249, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.787048816680908\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  127\n",
            "Avg loss:  tensor(0.2950, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7712364196777344\n",
            "Train  accuracy: 0.875\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  128\n",
            "Avg loss:  tensor(0.3083, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.96433162689209\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  129\n",
            "Avg loss:  tensor(0.2741, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7545361518859863\n",
            "Train  accuracy: 0.875\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  130\n",
            "Avg loss:  tensor(0.3436, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.767415761947632\n",
            "Train  accuracy: 0.8833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  131\n",
            "Avg loss:  tensor(0.2759, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.924764633178711\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  132\n",
            "Avg loss:  tensor(0.2473, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8362984657287598\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  133\n",
            "Avg loss:  tensor(0.2291, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8978028297424316\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  134\n",
            "Avg loss:  tensor(0.2821, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8497185707092285\n",
            "Train  accuracy: 0.8916666666666667\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  135\n",
            "Avg loss:  tensor(0.2624, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8747007846832275\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  136\n",
            "Avg loss:  tensor(0.2779, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8500144481658936\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  137\n",
            "Avg loss:  tensor(0.2410, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.831922769546509\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  138\n",
            "Avg loss:  tensor(0.2615, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7763493061065674\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  139\n",
            "Avg loss:  tensor(0.2522, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.906968593597412\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 118, 'loss': tensor(0.3770, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  140\n",
            "Avg loss:  tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.987199068069458\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  141\n",
            "Avg loss:  tensor(0.2819, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8281078338623047\n",
            "Train  accuracy: 0.875\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  142\n",
            "Avg loss:  tensor(0.2535, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.787662982940674\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.4166666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  143\n",
            "Avg loss:  tensor(0.2383, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.792102098464966\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  144\n",
            "Avg loss:  tensor(0.2744, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7558696269989014\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  145\n",
            "Avg loss:  tensor(0.2371, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.839820146560669\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  146\n",
            "Avg loss:  tensor(0.2475, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.099143743515015\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  147\n",
            "Avg loss:  tensor(0.2126, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8228487968444824\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.43333333333333335\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  148\n",
            "Avg loss:  tensor(0.2298, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7926077842712402\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  149\n",
            "Avg loss:  tensor(0.2181, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7854607105255127\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  150\n",
            "Avg loss:  tensor(0.2035, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8740596771240234\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  151\n",
            "Avg loss:  tensor(0.2451, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.832451105117798\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  152\n",
            "Avg loss:  tensor(0.2263, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.868411064147949\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  153\n",
            "Avg loss:  tensor(0.2215, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.811091899871826\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  154\n",
            "Avg loss:  tensor(0.1881, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9552247524261475\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  155\n",
            "Avg loss:  tensor(0.1940, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.894127130508423\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  156\n",
            "Avg loss:  tensor(0.2154, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8669867515563965\n",
            "Train  accuracy: 0.8833333333333333\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  157\n",
            "Avg loss:  tensor(0.2986, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8363547325134277\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.4\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  158\n",
            "Avg loss:  tensor(0.1845, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8404948711395264\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 140, 'loss': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  159\n",
            "Avg loss:  tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.894597053527832\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  160\n",
            "Avg loss:  tensor(0.2125, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.680659055709839\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  161\n",
            "Avg loss:  tensor(0.1847, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.697434425354004\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  162\n",
            "Avg loss:  tensor(0.2358, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.847851037979126\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  163\n",
            "Avg loss:  tensor(0.2389, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8166799545288086\n",
            "Train  accuracy: 0.9\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  164\n",
            "Avg loss:  tensor(0.1714, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.778892755508423\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 159, 'loss': tensor(0.2444, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.5833333333333334}\n",
            "Epoch:  165\n",
            "Avg loss:  tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.796532154083252\n",
            "Train  accuracy: 0.8916666666666667\n",
            "Validation  accuracy: 0.6166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  166\n",
            "Avg loss:  tensor(0.2126, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7979636192321777\n",
            "Train  accuracy: 0.8833333333333333\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  167\n",
            "Avg loss:  tensor(0.1833, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.778174638748169\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  168\n",
            "Avg loss:  tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9376721382141113\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  169\n",
            "Avg loss:  tensor(0.1615, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7763168811798096\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  170\n",
            "Avg loss:  tensor(0.1794, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.049508333206177\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  171\n",
            "Avg loss:  tensor(0.1442, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8392064571380615\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  172\n",
            "Avg loss:  tensor(0.1633, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.951439619064331\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  173\n",
            "Avg loss:  tensor(0.1870, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.862327814102173\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  174\n",
            "Avg loss:  tensor(0.1912, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.81848406791687\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  175\n",
            "Avg loss:  tensor(0.1818, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.846550226211548\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  176\n",
            "Avg loss:  tensor(0.1682, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8741836547851562\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  177\n",
            "Avg loss:  tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.855952262878418\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  178\n",
            "Avg loss:  tensor(0.1701, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.748142719268799\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  179\n",
            "Avg loss:  tensor(0.2145, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9266045093536377\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  180\n",
            "Avg loss:  tensor(0.1539, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.817742109298706\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  181\n",
            "Avg loss:  tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.776233673095703\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  182\n",
            "Avg loss:  tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.754277467727661\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  183\n",
            "Avg loss:  tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8962900638580322\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  184\n",
            "Avg loss:  tensor(0.1504, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.946570873260498\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  185\n",
            "Avg loss:  tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8524041175842285\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  186\n",
            "Avg loss:  tensor(0.1376, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.839428424835205\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  187\n",
            "Avg loss:  tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.897028684616089\n",
            "Train  accuracy: 0.9333333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  188\n",
            "Avg loss:  tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8270649909973145\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  189\n",
            "Avg loss:  tensor(0.1238, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8788461685180664\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  190\n",
            "Avg loss:  tensor(0.1454, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.915259599685669\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  191\n",
            "Avg loss:  tensor(0.1339, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.954976797103882\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  192\n",
            "Avg loss:  tensor(0.1505, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9161438941955566\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  193\n",
            "Avg loss:  tensor(0.1433, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8468401432037354\n",
            "Train  accuracy: 0.9083333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  194\n",
            "Avg loss:  tensor(0.2113, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.835099458694458\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  195\n",
            "Avg loss:  tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8909709453582764\n",
            "Train  accuracy: 1.0\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  196\n",
            "Avg loss:  tensor(0.1310, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8321266174316406\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  197\n",
            "Avg loss:  tensor(0.1227, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9265151023864746\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  198\n",
            "Avg loss:  tensor(0.1042, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7920100688934326\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  199\n",
            "Avg loss:  tensor(0.1481, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.890141248703003\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  200\n",
            "Avg loss:  tensor(0.1479, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.896810293197632\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  201\n",
            "Avg loss:  tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9255826473236084\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  202\n",
            "Avg loss:  tensor(0.1109, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.918335437774658\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  203\n",
            "Avg loss:  tensor(0.1239, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7002010345458984\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  204\n",
            "Avg loss:  tensor(0.0991, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8884003162384033\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  205\n",
            "Avg loss:  tensor(0.1223, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.776442050933838\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  206\n",
            "Avg loss:  tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.762324094772339\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  207\n",
            "Avg loss:  tensor(0.0884, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.714695453643799\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  208\n",
            "Avg loss:  tensor(0.1403, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.74873423576355\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  209\n",
            "Avg loss:  tensor(0.1206, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7008628845214844\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  210\n",
            "Avg loss:  tensor(0.0994, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9205315113067627\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  211\n",
            "Avg loss:  tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8789730072021484\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  212\n",
            "Avg loss:  tensor(0.1559, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9538373947143555\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  213\n",
            "Avg loss:  tensor(0.1468, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9131505489349365\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  214\n",
            "Avg loss:  tensor(0.1517, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.941303014755249\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  215\n",
            "Avg loss:  tensor(0.1271, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8008971214294434\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 165, 'loss': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6166666666666667}\n",
            "Epoch:  216\n",
            "Avg loss:  tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8034067153930664\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.6333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  217\n",
            "Avg loss:  tensor(0.0746, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.886169672012329\n",
            "Train  accuracy: 0.925\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  218\n",
            "Avg loss:  tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.811588764190674\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  219\n",
            "Avg loss:  tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.829138994216919\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  220\n",
            "Avg loss:  tensor(0.1040, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8099794387817383\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  221\n",
            "Avg loss:  tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8685531616210938\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  222\n",
            "Avg loss:  tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7399842739105225\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  223\n",
            "Avg loss:  tensor(0.0857, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.806156873703003\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  224\n",
            "Avg loss:  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.797865390777588\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  225\n",
            "Avg loss:  tensor(0.0831, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8660006523132324\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  226\n",
            "Avg loss:  tensor(0.0800, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.944225788116455\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  227\n",
            "Avg loss:  tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.85103440284729\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  228\n",
            "Avg loss:  tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.81457257270813\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  229\n",
            "Avg loss:  tensor(0.1027, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8312833309173584\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  230\n",
            "Avg loss:  tensor(0.1090, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9043118953704834\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.45\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  231\n",
            "Avg loss:  tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9207863807678223\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  232\n",
            "Avg loss:  tensor(0.1314, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7804582118988037\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  233\n",
            "Avg loss:  tensor(0.1191, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.80045747756958\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  234\n",
            "Avg loss:  tensor(0.1010, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9455697536468506\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  235\n",
            "Avg loss:  tensor(0.0751, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7793383598327637\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  236\n",
            "Avg loss:  tensor(0.1190, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.959883451461792\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  237\n",
            "Avg loss:  tensor(0.1015, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8660945892333984\n",
            "Train  accuracy: 1.0\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  238\n",
            "Avg loss:  tensor(0.1611, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.829174757003784\n",
            "Train  accuracy: 0.9166666666666666\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  239\n",
            "Avg loss:  tensor(0.0980, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8124842643737793\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  240\n",
            "Avg loss:  tensor(0.0972, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8848588466644287\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  241\n",
            "Avg loss:  tensor(0.0811, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.802673578262329\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  242\n",
            "Avg loss:  tensor(0.1005, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.847982406616211\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  243\n",
            "Avg loss:  tensor(0.0760, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8834598064422607\n",
            "Train  accuracy: 0.9416666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  244\n",
            "Avg loss:  tensor(0.0709, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8582375049591064\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  245\n",
            "Avg loss:  tensor(0.1055, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.024555683135986\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  246\n",
            "Avg loss:  tensor(0.1032, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.804069995880127\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  247\n",
            "Avg loss:  tensor(0.1453, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.799213409423828\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  248\n",
            "Avg loss:  tensor(0.1037, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.813544988632202\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  249\n",
            "Avg loss:  tensor(0.0843, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7777884006500244\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  250\n",
            "Avg loss:  tensor(0.1000, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8750925064086914\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  251\n",
            "Avg loss:  tensor(0.0838, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.748943328857422\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  252\n",
            "Avg loss:  tensor(0.1216, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.969696521759033\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  253\n",
            "Avg loss:  tensor(0.0756, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7916109561920166\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  254\n",
            "Avg loss:  tensor(0.1019, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8090736865997314\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  255\n",
            "Avg loss:  tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.874906539916992\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.6\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  256\n",
            "Avg loss:  tensor(0.0834, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.743041753768921\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  257\n",
            "Avg loss:  tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8018741607666016\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  258\n",
            "Avg loss:  tensor(0.0963, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.73396897315979\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  259\n",
            "Avg loss:  tensor(0.1013, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.918426752090454\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  260\n",
            "Avg loss:  tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8969995975494385\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  261\n",
            "Avg loss:  tensor(0.0929, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.806028366088867\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  262\n",
            "Avg loss:  tensor(0.0884, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8336517810821533\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  263\n",
            "Avg loss:  tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9096224308013916\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  264\n",
            "Avg loss:  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9143154621124268\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  265\n",
            "Avg loss:  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.892279863357544\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  266\n",
            "Avg loss:  tensor(0.1167, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8142848014831543\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  267\n",
            "Avg loss:  tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8806629180908203\n",
            "Train  accuracy: 1.0\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  268\n",
            "Avg loss:  tensor(0.0919, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9369211196899414\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  269\n",
            "Avg loss:  tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.76863169670105\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.6166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  270\n",
            "Avg loss:  tensor(0.0867, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.991682291030884\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  271\n",
            "Avg loss:  tensor(0.1316, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.949512243270874\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  272\n",
            "Avg loss:  tensor(0.1006, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.711881399154663\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  273\n",
            "Avg loss:  tensor(0.0758, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8058364391326904\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  274\n",
            "Avg loss:  tensor(0.1109, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8391590118408203\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  275\n",
            "Avg loss:  tensor(0.1057, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7919788360595703\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  276\n",
            "Avg loss:  tensor(0.0956, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.694152593612671\n",
            "Train  accuracy: 0.95\n",
            "Validation  accuracy: 0.55\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  277\n",
            "Avg loss:  tensor(0.1294, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.819659948348999\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  278\n",
            "Avg loss:  tensor(0.0807, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7981584072113037\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  279\n",
            "Avg loss:  tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.724496603012085\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  280\n",
            "Avg loss:  tensor(0.0819, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.0068747997283936\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  281\n",
            "Avg loss:  tensor(0.1292, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.879307985305786\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  282\n",
            "Avg loss:  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7021095752716064\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  283\n",
            "Avg loss:  tensor(0.0727, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8336892127990723\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  284\n",
            "Avg loss:  tensor(0.0705, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9897124767303467\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  285\n",
            "Avg loss:  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7604827880859375\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  286\n",
            "Avg loss:  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.783931255340576\n",
            "Train  accuracy: 1.0\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  287\n",
            "Avg loss:  tensor(0.0825, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.728769302368164\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  288\n",
            "Avg loss:  tensor(0.0855, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.863311529159546\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  289\n",
            "Avg loss:  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.748047113418579\n",
            "Train  accuracy: 0.9916666666666667\n",
            "Validation  accuracy: 0.4666666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  290\n",
            "Avg loss:  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.005065679550171\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  291\n",
            "Avg loss:  tensor(0.0769, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8964192867279053\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  292\n",
            "Avg loss:  tensor(0.0821, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8774478435516357\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  293\n",
            "Avg loss:  tensor(0.0723, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.868452787399292\n",
            "Train  accuracy: 0.9583333333333334\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  294\n",
            "Avg loss:  tensor(0.0698, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.808751106262207\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5833333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  295\n",
            "Avg loss:  tensor(0.0922, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7419445514678955\n",
            "Train  accuracy: 0.9833333333333333\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  296\n",
            "Avg loss:  tensor(0.0939, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.9830780029296875\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.48333333333333334\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  297\n",
            "Avg loss:  tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7619879245758057\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5333333333333333\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  298\n",
            "Avg loss:  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.7829067707061768\n",
            "Train  accuracy: 0.975\n",
            "Validation  accuracy: 0.5\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "Epoch:  299\n",
            "Avg loss:  tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.8085105419158936\n",
            "Train  accuracy: 0.9666666666666667\n",
            "Validation  accuracy: 0.5166666666666667\n",
            "Best val result:  {'epoch': 216, 'loss': tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.6333333333333333}\n",
            "1\n",
            "2\n",
            "3\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "Num training graphs:  540 ; Num validation graphs:  60\n",
            "Number of graphs:  600\n",
            "Number of edges:  37282\n",
            "Max, avg, std of graph size:  125 , 32.46 , 14.87\n",
            "Method: soft-assign\n",
            "Epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:71: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:73: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  m.bias.data = init.constant(m.bias.data, 0.0)\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:293: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
            "/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py:295: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  m.bias.data = init.constant(m.bias.data, 0.0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Avg loss:  tensor(1.7945, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.877264976501465\n",
            "Train  accuracy: 0.225\n",
            "Validation  accuracy: 0.26666666666666666\n",
            "Best val result:  {'epoch': 0, 'loss': tensor(1.7945, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.26666666666666666}\n",
            "Epoch:  1\n",
            "Avg loss:  tensor(1.7445, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  3.6710822582244873\n",
            "Train  accuracy: 0.25833333333333336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation  accuracy: 0.31666666666666665\n",
            "Best val result:  {'epoch': 1, 'loss': tensor(1.7445, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.31666666666666665}\n",
            "Epoch:  2\n",
            "Avg loss:  tensor(1.7037, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  4.074112176895142\n",
            "Train  accuracy: 0.3\n",
            "Validation  accuracy: 0.31666666666666665\n",
            "Best val result:  {'epoch': 2, 'loss': tensor(1.7037, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.31666666666666665}\n",
            "Epoch:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-a3d3bfca9541>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprog_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mbenchmark_task_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprog_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpkl_fname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a3d3bfca9541>\u001b[0m in \u001b[0;36mbenchmark_task_val\u001b[0;34m(args, writer, feat)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         _, val_accs = train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=None,\n\u001b[0;32m--> 504\u001b[0;31m             writer=writer)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mall_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mall_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a3d3bfca9541>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args, same_feat, val_dataset, test_dataset, writer, mask_nodes)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0massign_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assign_feats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massign_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soft-assign'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj, batch_num_nodes, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             embedding_tensor = self.gcn_forward(x, adj, \n\u001b[1;32m    349\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_first_after_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block_after_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     self.conv_last_after_pool[i])\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py\u001b[0m in \u001b[0;36mgcn_forward\u001b[0;34m(self, x, adj, conv_first, conv_block, conv_last, embedding_mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mx_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m#out_all = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/diffpool-master/encoders.py\u001b[0m in \u001b[0;36mapply_bn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m         ''' Batch normalization of 3D tensor x\n\u001b[1;32m    116\u001b[0m         '''\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mbn_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbn_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_features, eps, momentum, affine, track_running_stats)\u001b[0m\n\u001b[1;32m     79\u001b[0m                  track_running_stats=True):\n\u001b[1;32m     80\u001b[0m         super(_BatchNorm, self).__init__(\n\u001b[0;32m---> 81\u001b[0;31m             num_features, eps, momentum, affine, track_running_stats)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_features, eps, momentum, affine, track_running_stats)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[1;32m    607\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    166\u001b[0m                             \u001b[0;34m\"(torch.nn.Parameter or None required)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                             .format(torch.typename(param), name))\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             raise ValueError(\n\u001b[1;32m    170\u001b[0m                 \u001b[0;34m\"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-quDEOT8vDvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}